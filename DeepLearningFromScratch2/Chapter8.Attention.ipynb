{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 : Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "(5, 4)\n",
      "[[0.8  0.8  0.8  0.8 ]\n",
      " [0.1  0.1  0.1  0.1 ]\n",
      " [0.03 0.03 0.03 0.03]\n",
      " [0.05 0.05 0.05 0.05]\n",
      " [0.02 0.02 0.02 0.02]]\n",
      "====================\n",
      "(5, 4)\n",
      "[[ 1.00405895e+00  7.81975480e-01 -3.35264858e-01  2.00956928e+00]\n",
      " [-2.89881805e-02 -1.08980541e-01  5.01839930e-02  1.20355935e-01]\n",
      " [-1.11688985e-02 -1.98926037e-02  2.37432416e-02 -1.98794015e-02]\n",
      " [ 4.53713392e-02 -5.46465315e-02  1.89609484e-02 -5.08611291e-02]\n",
      " [ 8.61623341e-03 -1.46286002e-03 -4.73033949e-03 -3.31083302e-02]]\n",
      "====================\n",
      "(4,)\n",
      "[ 1.01788945  0.59699294 -0.24710701  2.02607636]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print('=' * 20)\n",
    "print(ar.shape)\n",
    "print(ar)\n",
    "\n",
    "t = hs * ar\n",
    "print('=' * 20)\n",
    "print(t.shape)\n",
    "print(t)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print('=' * 20)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "(10, 5, 4)\n",
      "==============================\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "\n",
    "t = hs * ar\n",
    "print(\"=\" * 30)\n",
    "print(t.shape)\n",
    "#print(t)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(\"=\" * 30)\n",
    "print(c.shape)\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "[ 0.25088412 -1.65062978  0.28562021  2.01612337  0.57194164]\n",
      "(10, 5)\n",
      "[0.10631236 0.01587694 0.11007013 0.62117996 0.14656061]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "print(s[0])\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * ds\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weight = None\n",
    "    \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weight = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weight.append(layer.attention_weight)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "        \n",
    "        return dhs_enc, dhs_dec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from seq2seq import *\n",
    "\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()  # Attention 레이어 \n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)  # context vector\n",
    "        out = np.concatenate((c, dec_hs), axis=2)  # context_vector & lstm h_t\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 151 / 351 | 시간 66[s] | 손실 1.62\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 132[s] | 손실 1.02\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "정확도 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 151 / 351 | 시간 74[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 145[s] | 손실 0.86\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "정확도 50.720%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 3 |  반복 151 / 351 | 시간 70[s] | 손실 0.12\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 139[s] | 손실 0.02\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.880%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 151 / 351 | 시간 70[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 133[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.940%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 151 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 127[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.960%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 151 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 128[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.960%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 151 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 124[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.960%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 151 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 139[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 151 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 142[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 151 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 140[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAG2CAYAAAB20iz+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA730lEQVR4nO3de3yT9d3/8XeSlpQeIaUtBSrnQzmIIgpiQZDquPHWjduxzY2fD9nm1E3mYB7GONXNDTYGt/NmQ9xud+umMHX3PM15qxxEUBB0KEjLQaggVNpSmraUHpJcvz9KIqEFCiS5riSv5+ORB/SbK8nnIqV59/qebIZhGAIAAIhBdrMLAAAACBeCDgAAiFkEHQAAELMIOgAAIGYRdAAAQMwi6AAAgJhF0AEAADErwewCzOTz+XT48GGlpaXJZrOZXQ4AAGgHwzBUW1urbt26yW4/+zWbuA46hw8fVl5entllAACAC3Dw4EH16NHjrMfEddBJS0uT1PIPlZ6ebnI1AACgPWpqapSXlxf4HD+buA46/u6q9PR0gg4AAFGmPcNOGIwMAABiFkEHAADELIIOAACIWQQdAAAQswg6AAAgZhF0AABAzCLoAACAmEXQAQAAMYugAwAAYpblgo7X69Ujjzyi0aNHn/U4wzC0ePFi9e7dW0lJSbr88sv1+uuvR6hKRBOvz9C7nxzVi9sO6d1PjsrrM8wu6YJwHtbCeVhPrJwL5xFaltkC4sSJE1q1apUWL16s3bt3a+jQoWc9/uc//7l+//vfa/ny5Ro4cKAef/xx3XTTTdqyZYsuvfTSCFUNq3ttR5keenmnytwNgbbcjCQtuGmwJg3NNbGy88N5WAvnYT2xci6cR+jZDMOwRFRct26dbr75Zv3gBz+Q1+vV66+/rm3btrV57LFjx9S9e3c9/fTTmjJlSqB9zJgx6tevn5566ql2vWZNTY0yMjLkdrvZ6yoGvbajTHf/5QOd/g3u3xll+bQRUfGDg/OwFs7DemLlXDiP9jufz2/LXNG54oorVFZWppSUFBUVFZ312Ndff10JCQm6+eabg9qnTp2qX//612GsEtHC6zP00Ms7W/1HkyRDLf/hHnp5p64f3FUO+7k3hTML52Et8Xgedpvk/3XYUMuwAf/jDEPyf3Xqr8z+9tMf98Xfv3ix9hxntBzY5ut6vIbmvfjxGc9Fkua/+LHyc9Mt/57Ew3mY8X/EMkGnPVut++3YsUODBw+Ww+EIah88eLA+//xz1dXVKTU1tdXjGhsb1djYGPi6pqbmwguGpb23vyrokunpDEll7gbd9F8blN4x4eQPTgX94D31h/oX95/69cnjjNN+QBuSL3Ccccrznv58XzxeZ7iv0eOT+0TzOc/j0of+Twl2e+B5Tr2/rS9ObT/T8UEfXKfcY5zheXSW432GobN1z/vPo/+cV9u1G7FZjDg7j74/fTViNYVTeW2jrl28zuwyLlosnIf/e+u9/VW6um9mRF7TMkHnfFRUVCgzs/U/kMvlktQSYNoKOgsXLtRDDz0U9vpgvvLaM4ecU+0si42we7zRK8lrdhkXzRf06370ipXzsAp/ZrRJgQBpO9nu8xnytuOfOsFus/yVEE87BuvGynm092d0KERl0PF4PLLbW08YC/wHOMNvUrNnz9asWbMCX9fU1CgvLy88RcJU2WlJ7TpuxnX9NCAnTTabZJPt5J/+H6ynfm0LtPuP1Wn32W1fHK9Wz9fO5z7ZrpNfb//MrZ/87/Zznsdvpg7XZXmdgj4Q/E79/xDcfsrfT7nnTBcigo4/z+f84MAxff/pD85yBi2Wf2uErujZ+ZzHmeX9T4/p7jg6jxXTRmhkL1fge1Q65ftfavk+P0sIafm77bTvnS8eH/j/IrV+jXZeEXv3k6O69Q+bznncn78zKmJXEC5EvJ1He39Gh0JUBp309HTt3r27VXt1dbVsNps6d277B4zT6ZTT6Qx3ebCAq3q7lJuRpM/dDW32Fdskdc1I0o8KB1j6t6NBXdP129V7znkeUy7vbunz+NKQru16P24YYu2xLTfE2XkUWnyskdT+/+tX9XZFurTzwnmEj+XW0WmPAQMGqKSkpFV7cXGx+vXrp6SkyCVFWJPDbtOCmwa3eZ//x/aCmwZb/of4qedxeqWcR+RxHtYTK+fCeYRPVAadG264QZWVlVqzZk1Q+3PPPacvf/nLJlUFq5k0NFffHdu7VXvXjKSomaYptZzH8mkj1DUjOMBzHubgPKwnVs6F8wgPy6yjc6qioiK98MILQevofP3rX9eoUaMCY2xuu+02rV+/Xo899pguueQSPf7441q5cqU++ugj5eTktOt1WEcn9s386zb9/V+HdNOluSocnKPstJZLplb/ragtXp+h9/ZXqby2gfOwAM7DemLlXDiPc4vKdXTOZdeuXerevXvg6xUrVuiBBx7QtGnTdOLECV1zzTVau3Ztu0MOYp/H69OaknJJ0v+7upfl+7bPxWG3WXoQYntxHtYSK+chxc65cB6hZckrOpHCFZ3YtnnfUX398U3qnJyoLXMKleCIyp5aAMBpzufzm5/8iFlvFh+RJE0YmE3IAYA4xU9/xKw3i1u6rQoH050JAPGKoIOY9ElFnfZXHlcHh13jBmSZXQ4AwCQEHcSkN3e2dFuN6uNSqjNqxtwDAEKMoIOYtPpkt9X1dFsBQFwj6CDmVB1v0tZPqyRJE/MJOgAQzwg6iDlrS8rlM6T83HR179TR7HIAACYi6CDmrC5pGZ9zfX62yZUAAMxG0EFMafR49dauCklMKwcAEHQQYzbtq9LxJq+y05wa2i3D7HIAACYj6CCmrD65GvLE/BzZo3ATPABAaBF0EDMMwwisn3P9YMbnAAAIOoghO8tqdNjdoKREu8b07WJ2OQAACyDoIGb4Fwkc2z9LSYkOk6sBAFgBQQcxw79b+fUsEggAOImgg5hwpKZBH33mls0mTRjE+BwAQAuCDmKCv9vqsrxOykpzmlwNAMAqCDqICf5uq0K6rQAApyDoIOrVN3m0YW+lJIIOACAYQQdRb8OeSjV5fMpzddSAnFSzywEAWAhBB1HP3201cVCObDZWQwYAfIGgg6jm8xlaU9IyEPl6NvEEAJyGoIOotu2zalXWNSktKUFX9XaZXQ4AwGIIOohq/r2trh2QpUQH384AgGB8MiCq+dfPodsKANAWgg6i1oGj9dp1pFYOu03jB7AaMgCgNYIOopZ/ttWVvTorIznR5GoAAFZE0EHUWl3CasgAgLMj6CAquU80a/O+KkmMzwEAnBlBB1Hprd0V8vgM9ctOVc/MFLPLAQBYFEEHUWk1m3gCANqBoIOo0+z1aW1gNWRmWwEAzoygg6izpbRKNQ0euVI66LK8zmaXAwCwMIIOoo5/kcDrBmXLYWcTTwDAmRF0EFUMwwisn1OYT7cVAODsCDqIKnvL6/Tp0Xp1cNg1tn+W2eUAACyOoIOo8ubJbqsx/TKV4kwwuRoAgNURdBBV/N1WE5lWDgBoB4IOokZlXaM+OHBMEuNzAADtQ9BB1FhbUi7DkIZ2T1duRkezywEARAGCDqJGoNtqEN1WAID2IeggKjQ0e/X2nkpJbOIJAGg/gg6iwrv7jqq+yauu6Uka0i3d7HIAAFGCoIOo8OZO/2yrbNlsrIYMAGgfgg4szzCMwLYPhXRbAQDOA0EHlvfx4Rp9XtOg5A4OXd0n0+xyAABRhKADy3vjZLfV2P5dlJToMLkaAEA0IejA8laX+DfxpNsKAHB+CDqwtDL3Ce04VCObTZowiNWQAQDnh6ADS/Nv4jniks7qkuo0uRoAQLQh6MDSVhfTbQUAuHAEHVjW8UaP3tl7VBKbeAIALgxBB5b19p4KNXl96pmZrH7ZqWaXAwCIQgQdWJZ/fE5hfg6rIQMALoilgs6mTZtUUFCg5ORk5ebmas6cOfJ4PG0e6/F4VFRUpEsuuUQpKSkaPXq03njjjQhXjHDx+gytKWkJOhPptgIAXCDLBJ3i4mIVFhZq7Nix2rp1q5YtW6bly5dr7ty5bR7/05/+VMuWLdNvfvMbbd68WYWFhZo8ebI2bdoU4coRDv86cExVx5uUnpSgK3u5zC4HABClbIZhGGYXIUm33nqr6uvr9eKLLwbaVqxYoZkzZ6q8vFypqcFjNDIzMzV37lzNnDkz0DZu3DgNHTpUv//979v1mjU1NcrIyJDb7VZ6OjtiW8mif5bosbc+0Zcv66bffuNys8sBAFjI+Xx+W+KKjtfr1SuvvKJp06YFtU+dOlUNDQ3auHFjq8c4HA6lpKQEtaWmpsrr9Ya1VkTGm8X+3cqZVg4AuHCWCDqlpaWqq6vTsGHDgtpdLpdycnK0d+/eVo+59957tWjRIm3btk3Nzc168skntX79en3/+98/4+s0NjaqpqYm6AbrKa08rr3ldUqw23TtgCyzywEARLEEswuQpIqKCkkt3VGnc7lccrvdrdp/+tOf6t1339Xll18um80mwzD03//93xo+fPgZX2fhwoV66KGHQlc4wsJ/NWdUH5cyOiaaXA0AIJpZ4oqOf2aV3d66HJvN1ubU4jvuuEP79+/X3/72N7333ntavHixZs2apZdffvmMrzN79my53e7A7eDBg6E7CYRMoNtqEN1WAICLY4krOv6BRG63u9VVnerq6lZt69ev11NPPaX9+/ere/fukqSRI0cqOTlZd955pyZNmqTExNZXApxOp5xO9kuyMnd9s7aUHpPEtg8AgItniSs6ffv2ld1uV0lJSVC72+1WWVmZhg4dGtT+7rvvqmfPnoGQ4zdu3DiVlZVp//79Ya8Z4bFud7m8PkMDc9J0SWay2eUAAKKcJYJOSkqKCgoKtGrVqqD2559/XtnZ2Ro1alRQe7du3XTgwAEdOXIkqH3Tpk1yOBzKzmaBuWj1xk7/bCveQwDAxbNE15UkzZs3T5MmTVJ+fr6mTJmi7du36/7779eSJUvkcDg0a9YsSdLSpUt1yy236OGHH9a///u/6+GHH1aPHj309ttv68EHH9Tdd9+tTp06mXsyuCBNHp/e2t0yML1wMN1WAICLZ5mgU1hYqGeeeUZFRUUqKipSr169tHjxYk2fPl2StG/fPjkcDklScnKyNm7cqAULFuiuu+5SeXm5+vbtq0WLFul73/uemaeBi7CltEq1DR51Se2gy3p0MrscAEAMsMzKyGZgZWRrKXrpY/3PO6X62sge+vVXz7xMAAAgvkXdysiAYRhaXdIyPofZVgCAUCHowBJ2H6nTwaoT6pBgV0H/LmaXAwCIEQQdWIJ/kcCCfl2U3MEyQ8cAAFGOoANL8Acduq0AAKFE0IHpymsbtO1gtSTWzwEAhBZBB6ZbW1Iuw5Au7ZGhnPQks8sBAMQQgg5M92ZxuSS6rQAAoUfQgakamr16e0/Lash0WwEAQo2gA1Nt3FuphmafumUkaXAuizYCAEKLoANTBbqtBufIZrOZXA0AINYQdGAan8/Q6mL/buWMzwEAhB5BB6bZcdit8tpGpXRwaHQfl9nlAABiEEEHpnlzZ8vVnGsHZsmZ4DC5GgBALCLowDRvnByfM3EQ3VYAgPAg6MAUh6pPqLisRnabNGEQ08oBAOFB0IEp/IOQr+jZWa6UDiZXAwCIVQQdmOKNnWziCQAIP4IOIq62oVmb9h2V1LJ+DgAA4ULQQcS9vadSzV5DvbukqG9WqtnlAABiGEEHEfdmoNuKQcgAgPAi6CCiPF6f1u5it3IAQGQQdBBRHxyo1rH6ZmV0TNQVPTubXQ4AIMYRdBBRb56cVn7doGwlOPj2AwCEF580iCh/0KHbCgAQCQQdRMwnFXXaV3FciQ6bxg3oYnY5AIA4QNBBxPhXQx7dJ1NpSYkmVwMAiAcEHUTMm8XMtgIARBZBBxFx7HiTtpZWSZImsn4OACBCCDqIiLW7yuUzpEFd09Sjc7LZ5QAA4gRBBxGx+mS31fXsbQUAiCCCDsKuyePTW7srJEkTGZ8DAIgggg7CbvP+o6pr9CgrzalLu2eYXQ4AII4QdBB2/k08Jw7Klt1uM7kaAEA8IeggrAzDYFo5AMA0BB2EVcnntTpUfUJJiXZd04/VkAEAkUXQQVj5u60K+nVRxw4Ok6sBAMQbgg7Cik08AQBmIuggbMprGvThZ25J0nWshgwAMAFBB2GzuqRlEPLwvE7KTksyuRoAQDwi6CBs/ONzrudqDgDAJAQdhMWJJq827K2UJBWy7QMAwCQEHYTFhr2VavT41L1TRw3MSTO7HABAnCLoICwC3VaDc2SzsRoyAMAcBB2EnM9nBAYiM60cAGAmgg5C7sPPqlVZ16g0Z4Ku6u0yuxwAQBwj6CDk/IsEjhuYpQ4JfIsBAMzDpxBCbvXJTTyvp9sKAGAygg5C6mBVvUo+r5XDbtP4gVlmlwMAiHMEHYTU6pPdViN7dlan5A4mVwMAiHcEHYTUm8XMtgIAWAdBByFT09CsTfuOSmI1ZACANRB0EDLrd1fI4zPUNytFvbukmF0OAAAEHYSOfzVkuq0AAFZB0EFIeLw+rd1VIYluKwCAdRB0EBJbPz0m94lmdU5O1IhLOptdDgAAkiwWdDZt2qSCggIlJycrNzdXc+bMkcfjOePxDQ0Nmjdvnvr06SOn06nc3Fw9++yzEawYfv5uqwmDsuWws4knAMAaEswuwK+4uFiFhYWaMWOGHn/8cRUXF+uOO+6Q1+vVokWLWh3v8Xh044036vjx41q2bJn69eunzz//XMnJySZUH98Mwwhs+8BqyAAAK7FM0PnZz36miRMnauHChZKkwYMHq7KyUjNnztTcuXOVmpoadPzvfvc7lZWV6f3331fHjh0lSQMGDIh43ZA+qTiu0qP16uCwa+wAVkMGAFiHJbquvF6vXnnlFU2bNi2oferUqWpoaNDGjRtbPebRRx/V7NmzAyGnPRobG1VTUxN0w8XzX80Z3TdTqU7LZGcAAKwRdEpLS1VXV6dhw4YFtbtcLuXk5Gjv3r1B7Xv37tW+ffs0cOBA3XTTTXK5XOrdu7d+/vOfy+fznfF1Fi5cqIyMjMAtLy8vLOcTb/zjc67Pzza5EgAAglki6FRUtExLzszMbHWfy+WS2+0OaisuLpbD4dCPfvQjfelLX9Lrr7+uWbNmadGiRYGur7bMnj1bbrc7cDt48GBoTyQOHa1r1AcHjkmSJjI+BwBgMZboZ/DPrLLbW+cum80mmy14Fk9NTY28Xq/uuusu3XbbbZKkkSNHqr6+Xr/4xS80e/bsNp/L6XTK6XSG4Qzi19pdFfIZ0uDcdHXr1P5uRAAAIsESV3TS09MlqdWVG0mqrq5udaUnMTFRkjR58uSg9sLCQtXW1urTTz8NU6U4XWA1ZBYJBABYkCWCTt++fWW321VSUhLU7na7VVZWpqFDhwa19+7dW1JLCDrV6Vd+EF4NzV6t39PS7ci0cgCAFVki6KSkpKigoECrVq0Kan/++eeVnZ2tUaNGBbVffvnl6tKli1auXBnU/o9//EPdu3dXr169wl0yJG3ad1T1TV7lpDs1tHu62eUAANCKJcboSNK8efM0adIk5efna8qUKdq+fbvuv/9+LVmyRA6HQ7NmzZIkLV26VAkJCZo/f75+/OMfKy0tTdddd53eeust/fKXv9SKFSu4shMh/mnlE/Nz+DcHAFiSZYJOYWGhnnnmGRUVFamoqEi9evXS4sWLNX36dEnSvn375HA4AsfPmDFDhmHot7/9rR544AH17dtXf/zjH/Wtb33LrFOIK4ZhaHVxuSSpkGnlAACLshmGYZhdhFlqamqUkZEht9sdGBCN9tlxyK1//68N6pjo0L/mX6+kRMe5HwQAQAicz+e3JcboIPr4r+aM7d+FkAMAsCyCDi6If3xOIbOtAAAWRtDBefvc3aDth9yy2aQJgxifAwCwLoIOztvqkparOZfndVJWGitNAwCsi6CD8+ZfDZm9rQAAVhfyoHPs2DGNGTNGcTyZK6bVN3m08ZOjkqTr2fYBAGBxIQ86TU1N2rx5M0EnRr29p1JNHp8ucSWrf3aq2eUAAHBW7Q46//u//6vm5uZW7T/84Q9VU1MT0qJgXV90W2WzGjIAwPLaHXSmTp2qY8eOtWr/3e9+p7q6upAWBWvy+gytKWlZP4dNPAEA0aDdQedMXVF0UcWPbQerdfR4k9KSEnRlb5fZ5QAAcE7MukK7+RcJHD8wW4kOvnUAANYXkk09P/30UzU0NEiSKioqQvGUsCD/+Bw28QQARIuQBJ2CggIZhiGbzRb4E7Hl06PHtae8Tgl2m8YPIOgAAKJDSILO+++/r9zcXElSeXm5LrvsslA8LSzkzZObeF7Zy6WM5ESTqwEAoH1CEnSys7OVk8MsnFgW6LZikUAAQBRp94hSm83WZpcU3VSxz13frPdKqyQxPgcAEF3afUXHMAwNHjyYYBOH1u0ul9dnqH92qnpmpphdDgAA7dbuoPPyyy+f8b4uXbqEpBhY0+qT43PotgIARJt2B50bb7wxnHXAgrw+Q+9+UqnXd34uSZowMMvkigAAOD8hX/WtQ4cOGjduHF1cUe61HWUq+NUaTfvv99TQ7JMk/XDlNr22o8zkygAAaL+QB53OnTtr7dq1BJ0o9tqOMt39lw9U5m4Iaj9S06C7//IBYQcAEDUuOOj89re/DWUdsAivz9BDL+9UWzuY+dseenmnvD72OAMAWN8FB50nn3wylHXAIt7bX9XqSs6pDEll7ga9t78qckUBAHCBzjkY+ciRIzpy5Eir9hMnTmj79u1Bu5dfcsklOnDgQNBxXbt2VXY2a69Ei/LaM4ecCzkOAAAznTPoPPLII/rVr37V5n3Dhw8P/N1ms+mJJ57Qt7/97UD4sdlsWrBggebPnx+ichFu2WlJIT0OAAAz2YxTL8lcpMrKSmVnZ8vn84XqKcOqpqZGGRkZcrvdSk9PN7scS/D6DBX8ao0+dze0OU7HJqlrRpI2PHidHHYGnAMAIu98Pr/bPUbn17/+tRobG9u8b9euXerfv/8Zt4lA9HDYbVpw0+AzhhxJWnDTYEIOACAqtDvozJ49W8ePH9eJEyd099136+6771Z9fb0kKSUlRdXV1ZKkEF4ggkkmDc3VqN6uVu1dM5K0fNoITRqaa0JVAACcv/Pa60qSFi1apI8++kgdO3bUT3/6Uz3yyCNKSkrS8ePHw1YkIsvrM7T7SK0kad6/56tLqlPZaUm6qreLKzkAgKjS7qDj75J67rnntGrVKmVkZKigoECPPPKIOnTooMbGRq7mxIhtB6t1rL5ZaUkJuu3qXkp0hHxdSQAAIuKcQeell15ScXGxpJarOvv379eAAQOUlJSkY8eO6Te/+Y2ampok6YxjeBBd1pa0bOJ57YAsQg4AIKqdM+h8+OGHWr9+vSTJ5/PJ4/EoIaHlYQkJCXr55Zdlt7d8GDY3NzMYOQasPhl0rhvE+kcAgOh2zqAzb948SZLD4ZDD4VBWVpYOHz6sTp06yW6366233grc7/F4wlstwq7MfULFZTWy2Vqu6AAAEM3aPUbH7/rrr9eSJUuUmpqqCRMmBNodDocMw9All1wS0gIRWWtLKiRJl+V1Umaq0+RqAAC4OOc962ru3Lm64YYblJCQoH/84x+B+x0Ohzp37qz9+/eHvkpEzJqT3VYT6bYCAMSAdgeda6+9VomJierfv/8ZwwyzrqJbQ7NXG/dWSpImEHQAADGg3UFn7dq1Z73/jTfeUGZm5kUXBPNs2ndUJ5q96pqepMG5bIkBAIh+5z1G50wKCgpC9VQwiX9a+YRBWcyeAwDEhJAvkrJ582Zt3rw51E+LMDMMQ2t2+aeV55hcDQAAoXHOoPNv//ZvganlZ7p97Wtfk8/n04033qirr75aY8aM0c033xw1u5hD2ltep4NVJ9Qhwa5r+tEFCQCIDefsunrkkUeCNuwcM2aMXnrpJWVlfbHGSmZmpv74xz/qww8/1McffyzDMHTDDTfoiSee0He/+92wFY/Q8c+2Gt0nU8kdQtajCQCAqc75iTZw4MBWbSNGjFC3bt2C2u68807Nnj1b+fn5kqQHHnhAK1euJOhECX/QuW4giwQCAGLHRY3RqaurC/x9x44dQQsIjh8/Xtu3b7+Yp0eEuE80a+unxyQxPgcAEFvaFXRGjBihY8daPgiHDBmixMRESdK9996r+fPnS5KOHTsW1J2VlZUV6PKCta3fXSGvz1C/7FRdkplsdjkAAIRMuwZjbNu2Tc3NzZIUuErz5z//WX/729+0devWlidKSFBDQ0PgMQ0NDYHNP2Fta9nEEwAQoy6o6+o///M/9f3vf1/PPvus+vXrJ0nq3r27SktLA8eUlpaqe/fuISkS4eP1GVq3u2V/K4IOACDWtPuSy6pVq1RdXa1nn31WdXV1eu2113TNNdcE7i8oKNDTTz+tsWPHSpJWrlzJIoJR4MPPqlV1vElpSQm6omdns8sBACCk2h10HnvsMVVUVOjo0aO67bbbNGTIkKD7f/jDH2r06NFKTEyUYRh66qmnWDgwCqwpbum2GjcgS4mOkK8fCQCAqdr9ybZu3TpVVFTonXfe0eHDh3XFFVfo448/Dtx/+eWXa9WqVXrnnXf07rvv6q9//auGDx8elqIROl9MK6fbCgAQe9p1RefUfY9Gjx6t119/XT/5yU80YcIErV+/XoMGDZIkTZkyRVOmTAlPpQi5z90N2llWI5tNGs/6OQCAGNSuoGMYRqu2RYsWqampSV/96lf1r3/9KzDlHNFj7cm9rS7L66TMVKfJ1QAAEHrtCjpr166Vy+Vq1b5w4UL16NGDkBOlVhfTbQUAiG3tCjrXXnttm+1Op1OzZs0KaUGIjIZmrzburZQkTWBaOQAgRjHNJk5t3l+lE81e5aQ7NaRbutnlAAAQFpYKOps2bVJBQYGSk5OVm5urOXPmyOPxnPNxHo9HAwYM0GWXXRb+ImPEqashnzrYHACAWGKZoFNcXKzCwkKNHTtWW7du1bJly7R8+XLNnTv3nI998skntWfPnghUGRsMw9DqkiOSpAmMzwEAxDCb0daUKhPceuutqq+v14svvhhoW7FihWbOnKny8nKlpqa2+biqqioNGTJEw4cP1+eff65t27a1+zVramqUkZEht9ut9PT46b7ZW16rwqXr1cFh17/mX68UJ3uSAQCix/l8flviio7X69Urr7yiadOmBbVPnTpVDQ0N2rhx4xkfe9999+nLX/6yRo8eHe4yY4Z/kcDRfTMJOQCAmGaJoFNaWqq6ujoNGzYsqN3lciknJ0d79+5t83EvvPCCXnvtNS1atKhdr9PY2KiampqgWzz6YjVkFgkEAMQ2SwSdioqW3bMzMzNb3edyueR2u1u1Hzx4UHfccYf+/Oc/q1OnTu16nYULFyojIyNwy8vLu6i6o5H7RLO2lB6TJF03KMfkagAACC9LBB3/zCq7vXU5Nput1ayghoYGffWrX9X3vvc9TZw4sd2vM3v2bLnd7sDt4MGDF1d4FHp7T4W8PkN9s1J0SWay2eUAABBWlhig4R9I5Ha7W13Vqa6ubtX2ne98R8nJyfrZz352Xq/jdDrldMb3Vgf+bquJ+VzNAQDEPksEnb59+8put6ukpER9+vQJtLvdbpWVlWno0KGBtk8//VTPPPOMJCkhoXX5NptNf/rTn3T77beHve5o4/UZemtXSzch08oBAPHAEkEnJSVFBQUFWrVqlSZPnhxof/7555Wdna1Ro0YF2nJzc/Wvf/2r1XM89thjWrt2rf7617/qkksuiUjd0ebDz6p19HiT0pISNLJXZ7PLAQAg7CwRdCRp3rx5mjRpkvLz8zVlyhRt375d999/v5YsWSKHwxHYU2vp0qVtroDctWtXdezYkdWRz8K/GvK4AVlKdFhieBYAAGFlmaBTWFioZ555RkVFRSoqKlKvXr20ePFiTZ8+XZK0b98+ORwOk6uMbl9MK6fbCgAQHyyzMrIZ4mll5M/dDRq9cLVsNmnrnEJlpsb3oGwAQPSKupWREX5rd7VczRneoxMhBwAQNwg6cSIwrXwQ3VYAgPhB0IkDDc1ebdxbKUmaQNABAMQRgk4c2Ly/SvVNXuWkOzWkW2yPRQIA4FQEnTjgn1Y+YWB2q+00AACIZQSdGGcYxhfTyum2AgDEGYJOjPuk4rgOVNWrg8Oua/p1MbscAAAiiqAT49aUHJEkjerjUorTMutDAgAQEQSdGEe3FQAgnhF0YlhNQ7O2lh6TRNABAMQngk4Me3t3pTw+Q32zUtQzM8XscgAAiDiCTgxbfXJ8DldzAADxiqATo3w+Q2/tqpDEasgAgPhF0IlRH35WraPHm5TmTNCVvVxmlwMAgCkIOjHKvxryuAFZSnTwNgMA4hOfgDFqtX/bB7qtAABxjKATg47UNOjjwzWy2aTxA7PMLgcAANMQdGKQv9tqeI9O6pLqNLkaAADMQ9CJQayGDABAC4JOjGn0eLVhb6Ukgg4AAASdGLN5X5Xqm7zKSXdqSLd0s8sBAMBUBJ0Y4++2mjAwWzabzeRqAAAwF0EnhhiG8UXQodsKAACCTiz5pOK4DlTVq4PDroJ+XcwuBwAA0xF0Yoh/WvmoPi6lOBNMrgYAAPMRdGII08oBAAhG0IkRNQ3N2lJaJYmgAwCAH0EnRry9u1Ien6E+WSnqmZlidjkAAFgCQSdG+LutJnI1BwCAAIJODPD5DL21m2nlAACcjqATAz465FZlXZPSnAm6spfL7HIAALAMgk4MWFN8RJI0dkAXJTp4SwEA8ONTMQas2eWfVp5jciUAAFgLQSfKHalp0I5DNbLZpPEDs8wuBwAASyHoRDn/asiX9uikLqlOk6sBAMBaCDpRLrAa8kBmWwEAcDqCThRr9Hi1YW+lJGliPkEHAIDTEXSi2Hv7q1Tf5FV2mlNDuqWbXQ4AAJZD0Iliq4tPLhI4MFs2m83kagAAsB6CTpQyDENr/dPK6bYCAKBNBJ0ota/yuD49Wq8ODrsK+nUxuxwAACyJoBOl/NPKR/VxKcWZYHI1AABYE0EnSp06PgcAALSNoBOFahqataW0ShLTygEAOBuCThTasKdSHp+hPlkp6pmZYnY5AABYFkEnCrEaMgAA7UPQiTI+n6F1gd3KCToAAJwNQSfKfHTIrcq6JqU5EzSyl8vscgAAsDSCTpTxd1uNHdBFHRJ4+wAAOBs+KaOMf/0cppUDAHBuBJ0oUl7ToO2H3JKk8QQdAADOiaATRfx7Ww3P66SsNKfJ1QAAYH0EnSjCtHIAAM4PQSdKNHq82rCnUhLTygEAaC9LBZ1NmzapoKBAycnJys3N1Zw5c+TxeNo89v3339fkyZOVnp4ul8ulyZMn6+OPP45wxZHz3v4qHW/yKivNqSHd0s0uBwCAqGCZoFNcXKzCwkKNHTtWW7du1bJly7R8+XLNnTu3zeNvueUWjRo1Shs2bNArr7win8+niRMn6ujRoxGuPDJO7bay220mVwMAQHSwGYZhmF2EJN16662qr6/Xiy++GGhbsWKFZs6cqfLycqWmpgYdX1xcrPz8/MDXdXV16tq1q37/+9/rtttua9dr1tTUKCMjQ263W+np1r5KMn7xWpUerddj067QpKFdzS4HAADTnM/ntyWu6Hi9Xr3yyiuaNm1aUPvUqVPV0NCgjRs3tnrMqSFHklJTU9WtWzcdOXIkrLWaYV9FnUqP1ivRYVNB/y5mlwMAQNSwRNApLS1VXV2dhg0bFtTucrmUk5OjvXv3nvM5Kisr9emnn2rIkCFnPKaxsVE1NTVBt2jg77Ya3SdTqc4Ek6sBACB6WCLoVFRUSJIyMzNb3edyueR2u8/6eMMwNGPGDA0aNEhf+tKXznjcwoULlZGREbjl5eVdXOERsobVkAEAuCCWCDr+mVV2e+tybDabbLYzD751u936yle+oq1bt+qll16Sw+E447GzZ8+W2+0O3A4ePHjxxYdZbUOz3ttfJYlp5QAAnC9LBB3/QKK2rtxUV1e3eaVHkrZu3arLL79chmHovffeU8+ePc/6Ok6nU+np6UE3q9uwp1Ien6E+XVLUq0uK2eUAABBVLBF0+vbtK7vdrpKSkqB2t9utsrIyDR06tNVj1q1bp4kTJ+ree+/VSy+9pM6dO0eq3Iha7Z9WztUcAADOmyWCTkpKigoKCrRq1aqg9ueff17Z2dkaNWpUUHt9fb2+8Y1v6Ne//rXuvffeSJYaUT6foXW7CDoAAFwoy0zhmTdvniZNmqT8/HxNmTJF27dv1/33368lS5bI4XBo1qxZkqSlS5fq7bffVkVFha699lqVlpYGPY/T6VRubq4JZxB62w+5VVnXpFRngkb2cpldDgAAUccyQaewsFDPPPOMioqKVFRUpF69emnx4sWaPn26JGnfvn2BgcZHjhyRz+drtZaOJF1xxRXaunVrRGsPF/9sq7H9u6hDgiUuvgEAEFUsszKyGay+MvJN/7VB2w+5tfirl2rqyOiYCg8AQLhF3crIaK28pkHbD7XMQhvP+jkAAFwQgo5FrdvVsoji8B4ZykpzmlwNAADRiaBjUatLWvbsmsBsKwAALhhBx4IaPV5t2FMpSZo4KMfkagAAiF4EHQvasv+Yjjd5lZXm1JBu1hskDQBAtCDoWNAXm3hmyW4/8z5fAADg7Ag6FrTm5PgcVkMGAODiEHQsZl9FnUqP1ivRYVNB/yyzywEAIKoRdCzG3201qnemUp2WWbgaAICoRNCxmLUnN/FkWjkAABePoGMhtQ3N2ryvShLjcwAACAWCjoVs2FMpj89Qny4p6t0lxexyAACIegQdCwlMK+dqDgAAIUHQsQifz9Dak/tb0W0FAEBoEHQsYvshtyrrGpXqTNCVvVxmlwMAQEwg6FiEv9tqbP8u6pDA2wIAQCjwiWoRTCsHACD0CDoWUF7boI8+c0uSxg9kNWQAAEKFoGMB60paBiEP75Gh7LQkk6sBACB2EHQsgGnlAACEB0HHZE0enzbsrZTEtHIAAEKNoGOyLaVVqmv0qEuqU0O7ZZhdDgAAMYWgY7LVxS3dVtcNypLdbjO5GgAAYgtBx2T+aeV0WwEAEHoEHRPtrzyu/ZXHleiwqaA/08oBAAg1go6J/LOtrurtUqozweRqAACIPQQdE60pOSJJum5QjsmVAAAQmwg6Jqlr9Oi9/VWSGJ8DAEC4EHRMsmFPhZq9hnp3SVHvLilmlwMAQEwi6JgksBryQK7mAAAQLgQdE/h8htac3N9qYj5BBwCAcCHomGDHYbcq6xqV6kzQlb1cZpcDAEDMIuiYwN9tVdCvizok8BYAABAufMqaYO3JoHMd3VYAAIQVQSfCymsb9OFnbknS+IGshgwAQDgRdCJs3a6WQciX9shQdlqSydUAABDbCDoRtpZp5QAARAxBJ4KaPD69vadSEtPKAQCIBIJOBG0prVJdo0ddUp0a2i3D7HIAAIh5BJ0I+mI15CzZ7TaTqwEAIPYRdCIoMK2cTTwBAIgIgk6E7K88rn2Vx5XosKmgfxezywEAIC4QdCLE3211VW+X0pISTa4GAID4QNCJEKaVAwAQeQSdCKhr9Gjz/qOSGJ8DAEAkEXQiYMOeCjV7DfXukqI+WalmlwMAQNwg6ETAGrqtAAAwBUEnzHw+Q2tP7m9FtxUAAJFF0Amzjw/XqKK2USkdHLqqt8vscgAAiCsEnTBbXXJEkjS2f5Y6JPDPDQBAJPHJG2ashgwAgHkIOmFUUduoDz9zS5LGD8oyuRoAAOIPQSeM1u1quZozrHuGstOSTK4GAID4Q9AJozV0WwEAYCpLBZ1NmzapoKBAycnJys3N1Zw5c+TxeM54/J/+9Cfl5+crKSlJgwYN0tNPPx3Bas/M6zP09u6KQNC5dgDdVgAAmMEyQae4uFiFhYUaO3astm7dqmXLlmn58uWaO3dum8c/9dRTmjFjhh588EFt27ZN06dP12233aZ//vOfEa482Gs7ylTwqzX6f0+8p0aPT5L0/ac/0Gs7ykytCwCAeGQzDMMwuwhJuvXWW1VfX68XX3wx0LZixQrNnDlT5eXlSk39YusEn8+nvLw83XfffZo5c2bQcxw6dEjr169v12vW1NQoIyNDbrdb6enpF30Or+0o091/+UCn/4PaTv65fNoITRqae9GvAwBAPDufz29LXNHxer165ZVXNG3atKD2qVOnqqGhQRs3bgxq37Jliw4fPqxvfetbQe1f+9rX9M4776i+vj7sNZ/O6zP00Ms7W4UcSYG2h17eKa/PErkSAIC4YImgU1paqrq6Og0bNiyo3eVyKScnR3v37g1q37Fjh3JycpSdHTzId/DgwfJ6vdq/f3+br9PY2KiampqgW6i8t79KZe6GM95vSCpzN+i9/VUhe00AAHB2lgg6FRUte0FlZma2us/lcsntdrc6/kzHSmp1vN/ChQuVkZERuOXl5V1s6QHltWcOORdyHAAAuHiWCDr+mVV2e+tybDabbDZbq+PPdOypf55u9uzZcrvdgdvBgwcvtvSA9q6Tw3o6AABEToLZBUgKDCRyu92trtRUV1e3aktPT2/zqk11dbWktq8MSZLT6ZTT6QxBxa1d1dul3Iwkfe5uaHOcjk1S14wkNvYEACCCLHFFp2/fvrLb7SopKQlqd7vdKisr09ChQ4PaBwwYoEOHDqm2tjaovbi4WB07dlSfPn3CXvPpHHabFtw0WNIXs6z8/F8vuGmwHPa2rzYBAIDQs0TQSUlJUUFBgVatWhXU/vzzzys7O1ujRo0Kah87dqySkpL03HPPBbU/++yzuvHGG5WQYM6FqklDc7V82gh1zQjunuqakcTUcgAATGCJritJmjdvniZNmqT8/HxNmTJF27dv1/33368lS5bI4XBo1qxZkqSlS5cqJSVFP/7xj3XfffcpNTVVl156qV544QX9/e9/1+bNm009j0lDc3X94K56b3+VymsblJ3W0l3FlRwAACLPMkGnsLBQzzzzjIqKilRUVKRevXpp8eLFmj59uiRp3759cjgcgeMXLFggm82mH/3oR6qqqtKIESP0f//3fxoyZIhZpxDgsNt0dd+2xwkBAIDIsczKyGYI9crIAAAg/KJuZWQAAIBwIOgAAICYRdABAAAxi6ADAABiFkEHAADELIIOAACIWQQdAAAQswg6AAAgZllmZWQz+NdKrKmpMbkSAADQXv7P7faseRzXQce/+3leXp7JlQAAgPNVW1urjIyMsx4T11tA+Hw+HT58WGlpabLZQrvpZk1NjfLy8nTw4EG2l7AA3g9r4f2wFt4P6+E9OTvDMFRbW6tu3brJbj/7KJy4vqJjt9vVo0ePsL5Geno636QWwvthLbwf1sL7YT28J2d2ris5fgxGBgAAMYugAwAAYhZBJ0ycTqcWLFggp9NpdikQ74fV8H5YC++H9fCehE5cD0YGAACxjSs6AAAgZhF0AABAzCLoAACAmEXQAQAAMYugEwabNm1SQUGBkpOTlZubqzlz5sjj8ZhdVlw6cuSIvvOd76hr165KSUnRyJEj9dJLL5ldFk769re/LZvNpurqarNLiXtPPvmkRowYoY4dO6pz58667777zC4pbv31r3/V8OHD1bFjRw0cOFCPPvpou/Z0QtviemXkcCguLlZhYaFmzJihxx9/XMXFxbrjjjvk9Xq1aNEis8uLOzNmzFBycrL+9re/KTU1VX/5y180ZcoUrV27VuPGjTO7vLi2e/duPfXUU2aXAUkLFizQH//4R/3iF7/Q6NGjdfz4cVVUVJhdVlx69dVX9c1vflMLFy7UjTfeqK1bt+qee+5Rc3OzfvzjH5tdXlRienmI3Xrrraqvr9eLL74YaFuxYoVmzpyp8vJypaammlhd/CkuLlZ+fn5Q2/jx49WnTx898cQTJlUFSbrhhhuUmJioV199VceOHVOnTp3MLikubdu2TVdffbU+/PBDDRgwwOxy4t4tt9wij8cT9Bkyf/58Pf/889q5c6eJlUUvuq5CyOv16pVXXtG0adOC2qdOnaqGhgZt3LjRpMri1+khR5IGDBigI0eOmFAN/J566imVlZVp5syZZpcS9x599FF985vfJORYhMPhUEpKSlBbamqqvF6vSRVFP4JOCJWWlqqurk7Dhg0Lane5XMrJydHevXtNqgynev/99zVkyBCzy4hbpaWlmjlzplasWKGEBHrPzfb6669r7NixmjFjhnJzc5Wdna3bb79dVVVVZpcWl37wgx/oxRdf1N///nd5PB5t2rRJS5cupdvqIhB0Qsjfp52ZmdnqPpfLJbfbHemScJrly5drz549uvvuu80uJS55PB7deuutuueeezRmzBizy4l7tbW1OnTokB599FElJibqhRde0PLly/XWW2/pG9/4htnlxaVrr71WRUVF+o//+A916NBBV199tW644QZ973vfM7u0qEXQCSH/zCq7vfU/q81mk81mi3RJOMnn86moqEgPPvignnvuOfXu3dvskuLSrFmz1KFDB82fP9/sUiCppqZGknTZZZdp6dKlGjVqlG655RatWrVKb7zxhrZs2WJyhfHnqaee0qJFi/S73/1OW7Zs0dNPP63169frJz/5idmlRS2uG4dQenq6JMntdre6qlNdXd3mlR6EX3l5ub75zW/q0KFDeueddzR06FCzS4pLf/jDH/Tss8/qgw8+kMPhMLscSEpMTJQkTZ48Oah91KhRSktL044dO3TllVeaUVpcqq2t1Q9/+EP94Q9/0NSpUyVJV1xxhS677DINHTpU3/rWt1oNjcC5cUUnhPr27Su73a6SkpKgdrfbrbKyMj5gTVBWVqYxY8aoe/fuev/993kPTPTLX/5SR44cUffu3QNXOCdMmCBJ6ty5s26//XZzC4xDWVlZSk1NbXMdI65AR97OnTvldrtbdesOHjxYLpdL7777rkmVRTeu6IRQSkqKCgoKtGrVqqDfkJ5//nllZ2dr1KhRJlYXn+68805deeWVevLJJ80uJe794x//UFNTU1Db1q1bdccdd2j9+vXq2bOnSZXFL5vNpsLCQq1cuVLf/e53A+1vv/22amtrdc0115hYXfzp1q2bJGnLli3q3r17oH3Pnj06evSocnNzzSotqhF0QmzevHmaNGmS8vPzNWXKFG3fvl3333+/lixZwuX6CKuvr9err76qJ554QqWlpa3u79mzJ7+1RtDgwYNbtfmvJAwbNox1dEwyd+5cjRkzRnfeeafuvPNOHTx4UPfcc49uv/12ppxHWF5enqZNm6a77rpLtbW1uuKKK7Rnzx49+OCDuvzyyzVp0iSzS4xKLBgYBs8++6yKior0ySefqFevXnrggQf0ne98x+yy4s6BAwfOepWgtraWBRxNtm7dOk2YMIEFA0325ptvavbs2froo4+UmZmp6dOn66GHHmL6vwmam5v16KOP6oknntD+/fuVm5urr3zlK5o/f74yMjLMLi8qEXQAAEDMYjAyAACIWQQdAAAQswg6AAAgZhF0AABAzCLoAACAmEXQAQAAMYugAwAAYhZBB0BMGT9+vBYtWmR2GQAsgqADIOrdddddZ90U9K677gpsJHqu2//8z/9ErG4A4cf63gAszzAMLViwQCtWrNDx48d14403avny5XK5XO16/KJFizR37tx2Hdve5wQQHbiiA8DyfvOb3+hPf/qT/vKXv+itt95SWVmZpk2b1u7Hd+rUST169NDDDz+spUuXqkePHkG3r3/96/rnP/+pHj16KDk5OYxnAiDSuKIDwNJ8Pp+WLFmiJUuW6Prrr5ckPfnkk+rTp4+Ki4uVn5/f7udKSkrS0aNHW7VXVFSwwSsQowg6ACxtz549OnLkiG6++eZAW+/evXXppZdqw4YN7Qo6dXV18ng86tChgyorK1VdXR10f3V1tRwOh6qrq+V0OtWxY8dQnwYAkxB0AFjaZ599poyMDKWlpQW19+jRQ+vXr1deXp4OHjyorKysMz7H+PHj9f777we+7ty5c6tjvv71r0uSfvCDH2jZsmUhqh6A2Qg6ACytublZKSkprdpTUlL05ptvqri4WAcOHNDkyZPP+Bxbt25t1TZp0iSNHDlSDz/8cEjrBWAtBB0AlpaRkdHmuJqjR4/qnnvu0Zw5c3TXXXepoaGh1TGNjY2qra1t83mbm5t14sQJVVZWtnl/586d5XA4Lq54AKYj6ACwtIEDB6qpqUm7du3SwIEDJbUMUN6xY4dmzJhx1seuXLlS06dPP+P9a9as0dKlS9u8r7i4WIMGDbrwwgFYAtPLAViay+XSuHHj9Ic//CHQ9tJLL6mhoUGFhYVnfeztt98uwzAu6EbIAWIDV3QAWN7SpUs1fvx4lZaWyuVyaeXKlXr00UfbPSX8hRde0JQpU8553DXXXKMNGzZcbLkALISgA8DyRowYoe3bt2vVqlWqr6/X6tWrddVVV7X78ZMmTdLBgwfPesyKFSu0du3aiy0VgMUQdABEhZ49e+rBBx+8oMcmJSWpR48eZz0mIyPjgp4bgLUxRgcAAMQsgg4AAIhZNsMwDLOLAIBQ8Xq9stvtstls5/W4AwcOqLy8XCNHjgxTZQDMQNABAAAxi64rAAAQswg6AAAgZhF0AABAzCLoAACAmEXQAQAAMYugAwAAYhZBBwAAxCyCDgAAiFkEHQAAELMIOgAAIGb9f6CyLhNwuLHIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad, eval_interval=150)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('TB_RECIPE_SEARCH-20231130.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184991, 18)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(data.columns)\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HYEONWOO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Recipe  Cluster\n",
      "0       어묵 | 김밥용김 | 당면 | 양파 | 당근 | 깻잎 | 튀김가루 | 올리브유 | ...        2\n",
      "1                        두부 | 당근 | 고추 | 브로콜리 | 새우 |  | 계란        2\n",
      "2       밥 | 당근 | 치자단무지 | 신김치 | 무순 | 날치알 | 김가루 |  | 참기름...        2\n",
      "3                                   현미 | 찹쌀 | 호두 | 물 | 소금        3\n",
      "4       북어포 | 찹쌀가루 1C [양념] 간장 | 설탕 | 물 | 다진파 | 다진마늘 | ...        4\n",
      "...                                                   ...      ...\n",
      "183989            | 스파게티면 | 명란젓 | 생크림 | 마늘 | 양파 | 소금 | 후추        3\n",
      "183990      | 스파게티면 | 관찰레 | 홀토마토 | 화이트와인 | 파르미지아노 치즈 | 소금        3\n",
      "183991            |  |  |  |  | 쪽파 | 양파 | 다진마늘 | 통깨 | 참기름        4\n",
      "183992  달걀 | 우유 | 물 | 맛살 | 당근 | 대파 1/3대 [양념] 소금 | 참치액 ...        3\n",
      "183993                계란 | 물 동량 | 참치액 | 쪽파 | 알새우 |  |  |         2\n",
      "\n",
      "[183994 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def preprocess_data(series):\n",
    "    ingredient_list = []\n",
    "    for data in series:\n",
    "        if pd.notnull(data):  # NaN이 아닌 경우에만 처리\n",
    "            # \"|\" 기호를 기준으로 문자열을 분리하여 각 재료와 해당 재료의 양을 추출\n",
    "            ingredients = data.split(\"|\")\n",
    "            recipe_list = []\n",
    "            for ingredient in ingredients:\n",
    "                # \"[재료]\" 문자열 제거\n",
    "                ingredient = ingredient.replace(\"[재료]\", \"\").strip()\n",
    "                parts = ingredient.split()\n",
    "                if len(parts) >= 1:  # 재료 문자열이 올바른 형식인 경우에만 처리\n",
    "                    ingredient_name = \" \".join(parts[:-1])  # 재료 이름 추출\n",
    "                    recipe_list.append(ingredient_name)\n",
    "            if recipe_list:  # 재료가 있는 경우에만 추가\n",
    "                ingredient_list.append(\" | \".join(recipe_list))  # 리스트를 문자열로 변환하여 추가\n",
    "    \n",
    "    return ingredient_list\n",
    "\n",
    "\n",
    "# 데이터 전처리\n",
    "recipes = preprocess_data(data[\"CKG_MTRL_CN\"])\n",
    "\n",
    "# 샘플 수와 클러스터 개수 확인\n",
    "num_samples = len(recipes)\n",
    "num_clusters = 5  # 클러스터 개수\n",
    "if num_samples >= num_clusters:\n",
    "    # 피처 벡터화\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(recipes)\n",
    "\n",
    "    # K-means 클러스터링 수행\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # 클러스터링 결과 확인\n",
    "    clustered_recipes = pd.DataFrame({\"Recipe\": recipes, \"Cluster\": kmeans.labels_})\n",
    "    print(clustered_recipes)\n",
    "else:\n",
    "    print(\"Error: Number of samples is less than number of clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP: 8\n",
      "파이썬: 6\n",
      "라이브러리: 5\n",
      "텍스트: 4\n",
      "분석: 4\n",
      "수: 3\n",
      "음성: 3\n",
      "및: 3\n",
      "CoreNLP: 3\n",
      "관련: 2\n",
      "작업: 2\n",
      "장단점: 2\n",
      "위: 2\n",
      "언어: 2\n",
      "그: 2\n",
      "러닝: 2\n",
      "로: 2\n",
      "도구: 1\n",
      "정말: 1\n",
      "용도: 1\n",
      "별: 1\n",
      "추천: 1\n",
      "Natural: 1\n",
      "Language: 1\n",
      "Processing: 1\n",
      "자연어: 1\n",
      "처리: 1\n",
      "AI: 1\n",
      "말: 1\n",
      "명령: 1\n",
      "번역: 1\n",
      "감성: 1\n",
      "요약: 1\n",
      "여타: 1\n",
      "애플리케이션: 1\n",
      "동안: 1\n",
      "크게: 1\n",
      "개선: 1\n",
      "사용: 1\n",
      "의: 1\n",
      "성능: 1\n",
      "또한: 1\n",
      "딥: 1\n",
      "통해: 1\n",
      "획기: 1\n",
      "향상: 1\n",
      "를: 1\n",
      "포함: 1\n",
      "모든: 1\n",
      "종류: 1\n",
      "머신: 1\n",
      "프론트엔드: 1\n",
      "제공: 1\n",
      "생태계: 1\n",
      "당황: 1\n",
      "것: 1\n",
      "사실: 1\n",
      "여기: 1\n",
      "주요: 1\n",
      "용: 1\n",
      "용례: 1\n",
      "인기: 1\n",
      "스탠포드: 1\n",
      "대학: 1\n",
      "Stanford: 1\n",
      "University: 1\n",
      "이: 1\n",
      "예측: 1\n",
      "대규모: 1\n",
      "수행: 1\n",
      "실용: 1\n",
      "단계: 1\n",
      "솔루션: 1\n",
      "자바: 1\n",
      "Java: 1\n",
      "작성: 1\n",
      "API: 1\n",
      "여러: 1\n",
      "패키지: 1\n",
      "등장: 1\n",
      "상태: 1\n",
      "Stanza: 1\n",
      "네이티: 1\n",
      "브: 1\n",
      "중: 1\n",
      "하나: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# KoBERT 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "# Konlpy 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 뉴스 기사 본문 입력\n",
    "text = \"\"\"\n",
    "    파이썬에는 NLP 관련 도구가 정말 많다. 원하는 텍스트 분석 작업에 적절한 파이썬 라이브러리는 어떻게 찾을 수 있을까? 용도별 추천 라이브러리와 장단점을 살펴본다. \n",
    "\n",
    "NLP(Natural Language Processing, 자연어 처리)는 ‘음성 및 텍스트를 위한 AI’라고 말할 수 있다. 음성 명령, 음성 및 텍스트 번역, 감성 분석, 텍스트 요약, 언어와 관련된 여타 많은 애플리케이션은 그 동안 크게 개선됐다. 분석에 사용되는 NLP의 성능 또한 딥러닝을 통해 획기적으로 향상됐다.\n",
    "\n",
    "파이썬 언어는 NLP를 포함한 모든 종류의 머신러닝에 유용한 프론트엔드를 제공한다. 하지만 파이썬 생태계에는 고를 NLP가 너무 많아 당황스러운 것도 사실이다. 여기서는 주요 파이썬용 라이브러리의 사용례와 장단점 그리고 인기도를 살펴본다. \n",
    " \n",
    "CoreNLP\n",
    "스탠포드 대학(Stanford University)이 만든 CoreNLP 라이브러리는 NLP 예측 및 분석 작업을 대규모로 수행할 수 있게 해주는 실용 단계의 NLP 솔루션이다. CoreNLP는 자바(Java)로 작성됐지만, 이를 위한 API와 여러 파이썬 패키지가 등장해 있는 상태다. Stanza로 불리는 네이티브 NLP 라이브러리가 그 중 하나다.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 형태소 분석 및 품사 태깅\n",
    "nouns = []\n",
    "for token in okt.pos(text):\n",
    "    # 명사만 추출\n",
    "    if token[1] in [\"Noun\", \"Alpha\"]:\n",
    "        nouns.append(token[0])\n",
    "\n",
    "# KoBERT 모델을 이용한 핵심 단어 추출\n",
    "inputs = tokenizer(\" \".join(nouns), return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "# 단어 빈도 계산\n",
    "word_counts = {}\n",
    "for word in nouns:\n",
    "    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "# 결과 출력\n",
    "for word, count in sorted(word_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.2.2+cpu\n",
      "Torchvision 버전: 0.17.2+cpu\n",
      "Torchaudio 버전: 2.2.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"Torchvision 버전: {torchvision.__version__}\")\n",
    "print(f\"Torchaudio 버전: {torchaudio.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# KoBERT 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "# 토큰화 및 모델 입력 준비\n",
    "inputs = []\n",
    "for index, row in data.iterrows():\n",
    "    text = \" \".join(row.dropna())  # NaN 값 제거 후 단어들을 하나의 문자열로 결합\n",
    "    inputs.append(tokenizer(text, return_tensors=\"pt\"))\n",
    "\n",
    "# 모델에 입력하고 예측 수행\n",
    "outputs = []\n",
    "for input_data in inputs:\n",
    "    output = model(**input_data)\n",
    "    outputs.append(output)\n",
    "\n",
    "# 결과 출력\n",
    "for output in outputs:\n",
    "    predictions = output.logits.argmax(dim=-1)\n",
    "    print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

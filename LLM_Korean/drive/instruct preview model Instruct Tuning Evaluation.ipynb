{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57f1d82110e44959352c392dacf7033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "1. 적절한 운동을 통해 신체의 균형을 맞추는 것\n",
      "2. 균일한 식사를 통해 영양소를 공급하는 것\n",
      "3. 충분한 수면을 취하는 것\n",
      "4. 적절한 스트레스 관리를 통해 심신의 균형을 맞추는 것\n",
      "5. 적절한 식습관을 통해 건강을 유지하는 것\n",
      "6. 적절한 운동을 통해 신체의 균형을 맞추는 것\n",
      "7. 적절한 수면을 취하는 것\n",
      "8. 적절한 스트레스 관리를 통해 심신의 균형을 맞추는 것\n",
      "9. 적절한 식습관을 통해 건강을 유지하는 것\n",
      "10. 적절한 운동을 통해 신체의 균형을 맞추는 것\n",
      "11. 적절한 수면을 취하는 것\n",
      "12. 적절한 스트레스 관리를 통해 심신의 균형을 맞추는 것\n",
      "13. 적절한 식습관을 통해 건강을 유지하는 것\n",
      "14. 적절한 운동을 통해 신체의 균형을 맞추는 것\n",
      "15. 적절한 수면을 취하는 것\n",
      "16. 적절한 스트레스 관리를 통해 심신의 균형을 맞추는 것\n",
      "17. 적절한 식습관을 통해 건강을 유지하는 것\n",
      "18. 적절한 운동을 통해 신체의 균형을 맞추는 것\n",
      "19. 적절한 수면을 취하는 것\n",
      "20. 적절한 스트레스 관리를 통해 심신의 균형을 맞추는 것\n",
      "21. 적절한 식습관을 통해 건강을 유지하는 것\n",
      "22. 적절한 운동을 통해 신체의 균형을 맞추는 것\n",
      "23. 적절한 수면을 취하는 것\n",
      "24. 적절한 스트레스 관리를 통해 심신의 균형을 맞추는 것\n",
      "25. 적절한 식습관을 통해 건강을 유지하는 것\n",
      "26. 적절한 운동을 통해 신체의 균형을 맞추는 것\n",
      "27. 적절한 수면을 취하는 것\n",
      "28. 적절한 스트레스 관리를 통해 심신의 균형을 맞추는 것\n",
      "29. 적절한 식습관을 통해 건강을 유지하는 것\n",
      "30. 적절한\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/Llama-3-Open-Ko-8B-Instruct-preview\", load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B-Instruct-preview\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction=\"건강을 유지하기 위한 3가지 팁을 알려줘\", # instruction\n",
    "        input=\"\", # input\n",
    "        output=\"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "gerenerated_output = tokenizer.batch_decode(outputs)\n",
    "\n",
    "index = gerenerated_output[0].index(\"Response:\")\n",
    "print(gerenerated_output[0][index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "The atomic components of an atom are protons, neutrons, and electrons. Protons are positively charged, neutrons are neutral, and electrons are negatively charged. The number of protons in an atom determines the element, and the number of neutrons and electrons determines the isotope. The number of protons and neutrons together determines the mass number. The number of electrons determines the electron configuration. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neutrons together. The atomic mass is the sum of the atomic masses of the protons, neutrons, and electrons. The atomic number is the number of protons and neut\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction=\"원자의 구성요소를 알려줘\", # instruction\n",
    "        input=\"\", # input\n",
    "        output=\"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "gerenerated_output = tokenizer.batch_decode(outputs)\n",
    "\n",
    "index = gerenerated_output[0].index(\"Response:\")\n",
    "print(gerenerated_output[0][index:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "피타고라스 정리는 기하학에서 삼각형의 내각과 외각의 길이의 비를 설명하는 원리입니다. 피타고라스 정리는 세 각의 길이의 비가 2:3:5라는 원칙을 설명합니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원리입니다. 피타고라스 정리는 세 각의 길이의 비가 2:3:5라는 원칙을 설명합니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라스 정리는 세 각의 길이의 비를 설명하는 원칙입니다. 피타고라\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction=\"피타고라스 정리에 대해서 알려줘\", # instruction\n",
    "        input=\"\", # input\n",
    "        output=\"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "gerenerated_output = tokenizer.batch_decode(outputs)\n",
    "\n",
    "index = gerenerated_output[0].index(\"Response:\")\n",
    "print(gerenerated_output[0][index:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruct Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b42333f3464cb5a2c1b45acb93167f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "건강을 유지하기 위한 3가지 팁은 다음과 같습니다. 첫째, 규칙적인 운동을 통해 신체를 건강하게 유지하세요. 둘째, 균형 잡힌 식단을 통해 영양소를 공급하세요. 셋째, 스트레스를 관리하세요.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"gamzadole/Alpaca_Fine_tuned_llama3\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"beomi/Llama-3-Open-Ko-8B-Instruct-preview\", load_in_4bit=True, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(base_model, \"gamzadole/Alpaca_Fine_tuned_llama3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B-Instruct-preview\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction=\"건강을 유지하기 위한 3가지 팁을 알려줘\", # instruction\n",
    "        input=\"\", # input\n",
    "        output=\"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "gerenerated_output = tokenizer.batch_decode(outputs)\n",
    "\n",
    "index = gerenerated_output[0].index(\"Response:\")\n",
    "print(gerenerated_output[0][index:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "원자는 양성자, 전자, 중성자로 구성됩니다.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction=\"원자의 구성요소를 알려줘\", # instruction\n",
    "        input=\"\", # input\n",
    "        output=\"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "gerenerated_output = tokenizer.batch_decode(outputs)\n",
    "\n",
    "index = gerenerated_output[0].index(\"Response:\")\n",
    "print(gerenerated_output[0][index:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "피타고라스 정리는 삼각형의 세 변의 길이와 내각의 크기를 이용하여 삼각형의 세 변의 길이를 구하는 방법입니다.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction=\"피타고라스 정리에 대해서 알려줘\", # instruction\n",
    "        input=\"\", # input\n",
    "        output=\"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "gerenerated_output = tokenizer.batch_decode(outputs)\n",
    "\n",
    "index = gerenerated_output[0].index(\"Response:\")\n",
    "print(gerenerated_output[0][index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "#include <cmath>\n",
      "using namespace std;\n",
      "int main() {\n",
      "    int n;\n",
      "    cout << \"피보나치 수열을 구현할 숫자를 입력하세요: \";\n",
      "    cin >> n;\n",
      "    vector<double> fib = {0.0, 1.0};\n",
      "    for (int i = 2; i <= n; i++) {\n",
      "        fib.push_back(fib[i-1] + fib[i-2]);\n",
      "    }\n",
      "    cout << \"피보나치 수열: \";\n",
      "    for (int i = 0; i < fib.size(); i++) {\n",
      "        cout << fib[i] << \" \";\n",
      "    }\n",
      "    return 0;\n",
      "}\n",
      "```<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        instruction=\"피보나치 수열을 C++로 구현해줘\", # instruction\n",
    "        input=\"\", # input\n",
    "        output=\"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 2000)\n",
    "gerenerated_output = tokenizer.batch_decode(outputs)\n",
    "\n",
    "index = gerenerated_output[0].index(\"Response:\")\n",
    "print(gerenerated_output[0][index:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

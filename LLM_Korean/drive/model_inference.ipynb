{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7d59ec91974a128ed75ca1983f19a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건강을 유지하기 위한 세 가지 팁을 알려주세요.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")   \n",
    "\n",
    "base_model = \"beomi/Llama-3-Open-Ko-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "messages = messages = [\n",
    "    {\"role\": \"system\", \"content\": \"친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.\"},\n",
    "    {\"role\": \"user\", \"content\": \"건강을 유지하기 위한 세 가지 팁을 알려주세요.\"},\n",
    "]\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번에 새로 나온 신제품을 수입하고 싶어서요.\n",
      "Ich möchte dieses Mal ein neues Produkt importieren.\n",
      "이번엔 ‘이재명 지키기’다. 이재명 경기지사가 4일 “대장동 개발사업은 공공개발”이라고 주장하자, 국민의힘은 “이 지사가 대장동 개발사업을 공공개발이라고 주장하는 것은 거짓말”이라고 반박했다. 이 지사는 이날 페이스북에 “대장동 개발사업은 공공개발”이라며 “민간이 개발이익을 독식하는 방식이 아니라 공공이 개발이익을 환수해 공공임대주택을 공급하고, 공공기여금을 도민을 위해 쓰는 공공개발”이라고 주장했다. 이 지사는 “민간이\n"
     ]
    }
   ],
   "source": [
    "prompt = \"세 가지 기본 색은 무엇인가요?\"\n",
    "result = pipe(prompt)\n",
    "\n",
    "print(result[0]['generated_text'][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "야 한다.이번에 새로 나온 신제품을 수입하고 싶어서요.\n",
      "Ich möchte dieses Mal ein neues Produkt importieren.\n",
      "이번엔 ‘이재명 지키기’다. 이재명 경기지사가 4일 “대장동 개발사업은 공공개발”이라고 주장하자, 국민의힘은 “이 지사가 거짓말을 하고 있다”고 반박했다. 대장동 개발사업은 이 지사가 성남시장 시절 추진한 사업이다. 이 지사는 이날 페이스북에 “대장동 개발사업은 공공개발로, 민간이익을 환수해 공공이익으로 환원하는 공공개발의 모범사례”라고 주장했다. 이 지사는 “민간이익을 환수해 공공이익으로 환원하는 공공개발\n"
     ]
    }
   ],
   "source": [
    "prompt = \"원자 구조를 설명해줘\"\n",
    "result = pipe(prompt)\n",
    "\n",
    "print(result[0]['generated_text'][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd474d6862944d483737148a11554a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "세 가지 기본 색은 무엇입니까?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"beomi/Llama-3-Open-Ko-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model.eval()\n",
    "PROMPT = \"당신은 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 한국어로 답변해야합니다.\"\n",
    "instrcution = \"세 가지 기본 색은 무엇인가요?\"\n",
    "\n",
    "messages = [{\"role\":\"system\", \"content\": f\"{PROMPT}\"},\n",
    "            {\"role\":\"user\", \"content\":f\"{instrcution}\"}]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "outputs = model.generate(input_ids, max_new_tokens=512, eos_token_id=terminators, do_sample=True, temperature = 1, top_p=0.9,)\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad1f85b9200462696d470010ffc9109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "색이론에서 기본 색은 RGB 색상 모델에서 레드(Red), 그린(Green), 블루(Blue)입니다. 이 기본 색은 다른 색을 생성하는 데 사용됩니다. 보통 0에서 255까지 숫자로 표시됩니다. 이 숫자는 색의 밝기와 조합을 나타냅니다. 예를 들어, 레드는 255를 나타내고 그린은 0을 나타냅니다. 블루는 128을 나타냅니다. 이 기본 색은 다양한 색을 생성하는 데 사용됩니다. 따라서, 이 기본 색을 조합하면 다양한 색을 만들 수 있습니다. 다양한 색을 생성하는 방법은 너무 많아서 모든 색을 만들어낼 수 있습니다. 색이론에서 기본 색은 가장 기본적인 색을 지칭합니다. 또한 이 기본 색은 다른 색을 생성하는 데 사용됩니다. 다양한 색을 생성하는 방법은 너무 많아서 모든 색을 만들어낼 수 있습니다. 색이론에서 기본 색은 가장 기본적인 색을 지칭합니다. \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = 'beomi/Llama-3-Open-Ko-8B-Instruct-preview'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model.eval()\n",
    "PROMPT = \"당신은 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 한국어로 답변해야합니다.\"\n",
    "instrcution = \"세 가지 기본 색은 무엇인가요?\"\n",
    "\n",
    "messages = [{\"role\":\"system\", \"content\": f\"{PROMPT}\"},\n",
    "            {\"role\":\"user\", \"content\":f\"{instrcution}\"}]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "outputs = model.generate(input_ids, max_new_tokens=512, eos_token_id=terminators, do_sample=True, temperature = 1, top_p=0.9,)\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

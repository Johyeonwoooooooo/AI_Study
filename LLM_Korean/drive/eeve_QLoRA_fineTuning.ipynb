{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEVE Fine-Tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Gamzadole\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"yanolja/EEVE-Korean-10.8B-v1.0\"\n",
    "kr_dataset = \"Bingsu/ko_alpaca_data\"\n",
    "\n",
    "new_model = \"Llama3-Ko-3-8B-fine_tuning_jo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd31fe31b1c3436f931d78bff05ab7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODEL = \"yanolja/EEVE-Korean-10.8B-v1.0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1) ë§¤ì¼ ì•„ì¹¨, ì €ë…ì— ë°˜ë“œì‹œ ì–‘ì¹˜ì§ˆì„ í•˜ê³  ì¹˜ê°„ ì¹«ì†”ì´ë‚˜ ì¹˜ì‹¤ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
      "2) ì •ê¸°ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ì„ ë°›ìœ¼ì„¸ìš”.\n",
      "3) ìˆ ê³¼ ë‹´ë°°ëŠ” í”¼í•˜ê³  ìŒì‹ë¬¼ì„ ì„­ì·¨í•œ í›„ì—” ë°”ë¡œ ì…ì•ˆì„ ë¬¼ë¡œ ì¶©ë¶„íˆ ì ì…”ì£¼ì„¸ìš”.\n",
      "Q5. ë§ˆì§€ë§‰ìœ¼ë¡œ êµ¬ê°•ê´€ë¦¬ì™€ ê´€ë ¨ëœ í•œë§ˆë”” ë¶€íƒë“œë¦½ë‹ˆë‹¤.\n",
      "êµ¬ê°•ì§ˆí™˜ì€ ì˜ˆë°©ì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ì •ê¸°ì ì¸ ê²€ì§„ì„ í†µí•´ ìì‹ ì˜ ìƒíƒœë¥¼ íŒŒì•…í•˜ê³  ì ì ˆíˆ ëŒ€ì²˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ê±´ê°•ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì„¸ ê°€ì§€ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì •\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256) # max_new_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True, # ìƒ˜í”Œë§ ì „ëµ ì‚¬ìš©. í™•ë¥  ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ì„ íƒ\n",
    "    temperature=0.2, # ìƒ˜í”Œë§ì˜ ë‹¤ì–‘ì„±ì„ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°. ê°’ì´ ë†’ì„ìˆ˜ë¡ ëœë¤ì„± ì¦ê°€\n",
    "    top_k=50, # ë‹¤ìŒ í† í°ì„ ì„ íƒí•  ë•Œ ìƒìœ„ kê°œì˜ í›„ë³´ í† í° ì¤‘ì—ì„œ ì„ íƒ. ì—¬ê¸°ì—ì„œëŠ” ìƒìœ„ 50ê°œì˜ í›„ë³´ í† í° ì¤‘ì—ì„œ ìƒ˜í”Œë§\n",
    "    top_p=0.95, # ëˆ„ì  í™•ë¥ ì´ pê°€ ë  ë•Œê¹Œì§€ í›„ë³´ í† í°ì„ í¬í•¨\n",
    "    repetition_penalty=1.2, # ë°˜ë³µ íŒ¨ë„í‹°ë¥¼ ì ìš©í•˜ì—¬ ê°™ì€ ë‹¨ì–´ë‚˜ êµ¬ì ˆì´ ë°˜ë³µë˜ëŠ” ê²ƒ ë°©ì§€\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):]) # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì´í›„ì— ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 49620\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_MODEL = \"yanolja/EEVE-Korean-10.8B-v1.0\"\n",
    "dataset_koalpaca = load_dataset(kr_dataset)\n",
    "dataset_koalpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'ê±´ê°•ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì„¸ ê°€ì§€ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”.', 'input': '', 'output': 'ì„¸ ê°€ì§€ íŒì€ ì•„ì¹¨ì‹ì‚¬ë¥¼ ê¼­ ì±™ê¸°ë©°, ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ê³ , ì ê·¹ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "df_koalpaca = pd.DataFrame(dataset_koalpaca['train'])\n",
    "\n",
    "# ì¤‘ë³µ ì œê±°\n",
    "df_koalpaca = df_koalpaca.drop_duplicates(keep='first', ignore_index=True)\n",
    "\n",
    "# HuggingFace Dataset í˜•íƒœë¡œ ë³€í™˜\n",
    "dataset_koalpaca = Dataset.from_pandas(df_koalpaca)\n",
    "\n",
    "print(dataset_koalpaca[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285f4d9c402b4096889a837db9ad30ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NF4 ì–‘ìí™”ë¥¼ ìœ„í•œ ì„¤ì •\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # ëª¨ë¸ì„ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë¡œë“œ\n",
    "    bnb_4bit_quant_type=\"nf4\", # 4ë¹„íŠ¸ NormalFloat ì–‘ìí™”: ì–‘ìí™”ëœ íŒŒë¼ë¯¸í„°ì˜ ë¶„í¬ ë²”ìœ„ë¥¼ ì •ê·œë¶„í¬ ë‚´ë¡œ ì–µì œí•˜ì—¬ ì •ë°€ë„ ì €í•˜ ë°©ì§€\n",
    "    bnb_4bit_use_double_quant=True, # ì´ì¤‘ ì–‘ìí™”: ì–‘ìí™”ë¥¼ ì ìš©í•˜ëŠ” ì •ìˆ˜ì— ëŒ€í•´ì„œë„ ì–‘ìí™” ì ìš©\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # ì—°ì‚° ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš© (default: torch.float32)\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input_template = \"\"\"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œì‚¬í•­ê³¼ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ì§ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­:\n",
    "{instruction}\n",
    "\n",
    "### ì…ë ¥:\n",
    "{input}\n",
    "\n",
    "### ì‘ë‹µ:\"\"\"\n",
    "\n",
    "\n",
    "prompt_no_input_template = \"\"\"ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œì‚¬í•­ì…ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "### ì§€ì‹œì‚¬í•­:\n",
    "{instruction}\n",
    "\n",
    "### ì‘ë‹µ:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716e067335764fc7a869a727a42ad816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_prompt(data_point):\n",
    "  instruction = data_point[\"instruction\"]\n",
    "  input = data_point[\"input\"]\n",
    "  output = data_point[\"output\"]\n",
    "\n",
    "  if input:\n",
    "    res = prompt_input_template.format(instruction=instruction, input=input)\n",
    "  else:\n",
    "    res = prompt_no_input_template.format(instruction=instruction)\n",
    "\n",
    "  if output:\n",
    "    res = f\"{res}{output}<|im_end|>\" # eos_tokenì„ ë§ˆì§€ë§‰ì— ì¶”ê°€\n",
    "\n",
    "  data_point['text'] = res\n",
    "\n",
    "  return data_point\n",
    "\n",
    "# ë°ì´í„°ì…‹ì— í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "remove_column_keys = dataset_koalpaca.features.keys() # ê¸°ì¡´ ì»¬ëŸ¼(instruction, output ë“±) ì œê±°\n",
    "dataset_cvted = dataset_koalpaca.select(range(1)).shuffle().map(generate_prompt, remove_columns=remove_column_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ë˜ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œì‚¬í•­ì…ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
      "\n",
      "### ì§€ì‹œì‚¬í•­:\n",
      "ê±´ê°•ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì„¸ ê°€ì§€ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "\n",
      "### ì‘ë‹µ:ì„¸ ê°€ì§€ íŒì€ ì•„ì¹¨ì‹ì‚¬ë¥¼ ê¼­ ì±™ê¸°ë©°, ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ê³ , ì ê·¹ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    if \"ê±´ê°•\" in dataset_cvted[i][\"text\"]:\n",
    "        print(dataset_cvted[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d11842fa97a4de182415b98a8fb00e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "  outputs = tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "  return outputs\n",
    "\n",
    "remove_column_keys = dataset_cvted.features.keys()\n",
    "dataset_tokenized = dataset_cvted.map(tokenize_function, batched=True, remove_columns=remove_column_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,555,904 || all params: 10,807,480,320 || trainable%: 0.0236\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4, # LoRA ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ rank. ì •ìˆ˜í˜•ì´ë©° ê°’ì´ ì‘ì„ìˆ˜ë¡ trainable parameterê°€ ì ì–´ì§\n",
    "    lora_alpha=8, # LoRA ìŠ¤ì¼€ì¼ë§ íŒ©í„°. ì¶”ë¡  ì‹œ PLM weightì™€ í•©ì¹  ë•Œ LoRA weightì˜ ìŠ¤ì¼€ì¼ì„ ì¼ì •í•˜ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©\n",
    "    lora_dropout=0.05,\n",
    "    bias='none', # bias íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµì‹œí‚¬ì§€ ì§€ì •. ['none', 'all', 'lora_only']\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# ì–‘ìí™”ëœ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ì „, ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ í˜¸ì¶œ\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# LoRA í•™ìŠµì„ ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ peftë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ wrapping í•´ì£¼ì–´ì•¼ í•¨\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# í•™ìŠµ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(examples):\n",
    "    examples_batch = tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "    examples_batch['labels'] = examples_batch['input_ids'] # ëª¨ë¸ í•™ìŠµ í‰ê°€ë¥¼ ìœ„í•œ loss ê³„ì‚°ì„ ìœ„í•´ ì…ë ¥ í† í°ì„ ë ˆì´ë¸”ë¡œ ì‚¬ìš©\n",
    "    return examples_batch\n",
    "\n",
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=4, # ê° ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ. ì‘ì„ìˆ˜ë¡(1~2) ì¢€ ë” ë¹ ë¥´ê²Œ alignment ë¨\n",
    "    gradient_accumulation_steps=1, \n",
    "    max_steps=-1, \n",
    "    learning_rate=1e-3, # í•™ìŠµë¥ \n",
    "    bf16=True, # bf16 ì‚¬ìš© (ì§€ì›ë˜ëŠ” í•˜ë“œì›¨ì–´ í™•ì¸ í•„ìš”)\n",
    "    optim=\"paged_adamw_8bit\", # 8ë¹„íŠ¸ AdamW ì˜µí‹°ë§ˆì´ì €\n",
    "    logging_steps=1, # ë¡œê¹… ë¹ˆë„\n",
    "    output_dir = \"./output\",\n",
    "    num_train_epochs=10, # epoch\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_tokenized,\n",
    "    args=train_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26963f6b81a14ef3b2339c4f8045119c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3856, 'grad_norm': 2.5717580318450928, 'learning_rate': 0.0009000000000000001, 'epoch': 1.0}\n",
      "{'loss': 1.79, 'grad_norm': 3.1512644290924072, 'learning_rate': 0.0008, 'epoch': 2.0}\n",
      "{'loss': 1.5235, 'grad_norm': 6.877908706665039, 'learning_rate': 0.0007, 'epoch': 3.0}\n",
      "{'loss': 0.9257, 'grad_norm': 4.507090091705322, 'learning_rate': 0.0006, 'epoch': 4.0}\n",
      "{'loss': 0.5123, 'grad_norm': 2.57369065284729, 'learning_rate': 0.0005, 'epoch': 5.0}\n",
      "{'loss': 0.2707, 'grad_norm': 2.146669387817383, 'learning_rate': 0.0004, 'epoch': 6.0}\n",
      "{'loss': 0.2125, 'grad_norm': 4.079265117645264, 'learning_rate': 0.0003, 'epoch': 7.0}\n",
      "{'loss': 0.1481, 'grad_norm': 1.5603268146514893, 'learning_rate': 0.0002, 'epoch': 8.0}\n",
      "{'loss': 0.1193, 'grad_norm': 0.25185197591781616, 'learning_rate': 0.0001, 'epoch': 9.0}\n",
      "{'loss': 0.1159, 'grad_norm': 0.19507254660129547, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 19.483, 'train_samples_per_second': 0.513, 'train_steps_per_second': 0.513, 'train_loss': 0.8003613464534283, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.8003613464534283, metrics={'train_runtime': 19.483, 'train_samples_per_second': 0.513, 'train_steps_per_second': 0.513, 'total_flos': 45963539251200.0, 'train_loss': 0.8003613464534283, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "FINETUNED_MODEL = \"eeve\"\n",
    "trainer.model.save_pretrained(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yanolja/EEVE-Korean-10.8B-v1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6734653a9a141f5b65b79fd40215b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig\n",
    "\n",
    "FINETUNED_MODEL = \"eeve\"\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # ëª¨ë¸ì„ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë¡œë“œ\n",
    "    bnb_4bit_quant_type=\"nf4\", # 4ë¹„íŠ¸ NormalFloat ì–‘ìí™”: ì–‘ìí™”ëœ íŒŒë¼ë¯¸í„°ì˜ ë¶„í¬ ë²”ìœ„ë¥¼ ì •ê·œë¶„í¬ ë‚´ë¡œ ì–µì œí•˜ì—¬ ì •ë°€ë„ ì €í•˜ ë°©ì§€\n",
    "    bnb_4bit_use_double_quant=True, # ì´ì¤‘ ì–‘ìí™”: ì–‘ìí™”ë¥¼ ì ìš©í•˜ëŠ” ì •ìˆ˜ì— ëŒ€í•´ì„œë„ ì–‘ìí™” ì ìš©\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # ì—°ì‚° ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš© (default: torch.float32)\n",
    ")\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(FINETUNED_MODEL)\n",
    "print(peft_config.base_model_name_or_path)\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path\n",
    ")\n",
    "\n",
    "# QLoRA ëª¨ë¸ ë¡œë“œ\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL, torch_dtype=torch.bfloat16)\n",
    "# QLoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gamzadole\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê±´ê°•ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì„¸ ê°€ì§€ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”.\n",
      "A: ì²«ì§¸, í•­ìƒ ì¢‹ì€ ìì„¸ë¥¼ ìœ ì§€í•˜ì„¸ìš”. ë‘˜ì§¸, ìš´ë™ì€ í•„ìˆ˜ì…ë‹ˆë‹¤! ë§ˆì§€ë§‰ìœ¼ë¡œ, ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ê±´ê°•ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ì„¸ ê°€ì§€ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ ì„¤ì •\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True, # ìƒ˜í”Œë§ ì „ëµ ì‚¬ìš©. í™•ë¥  ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ì„ íƒ\n",
    "    temperature=0.2, \n",
    "    top_k=50, # ë‹¤ìŒ í† í°ì„ ì„ íƒí•  ë•Œ ìƒìœ„ kê°œì˜ í›„ë³´ í† í° ì¤‘ì—ì„œ ì„ íƒ. ì—¬ê¸°ì—ì„œëŠ” ìƒìœ„ 50ê°œì˜ í›„ë³´ í† í° ì¤‘ì—ì„œ ìƒ˜í”Œë§\n",
    "    top_p=0.95, # ëˆ„ì  í™•ë¥ ì´ pê°€ ë  ë•Œê¹Œì§€ í›„ë³´ í† í°ì„ í¬í•¨\n",
    "    repetition_penalty=1.2, # ë°˜ë³µ íŒ¨ë„í‹°ë¥¼ ì ìš©í•˜ì—¬ ê°™ì€ ë‹¨ì–´ë‚˜ êµ¬ì ˆì´ ë°˜ë³µë˜ëŠ” ê²ƒ ë°©ì§€\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

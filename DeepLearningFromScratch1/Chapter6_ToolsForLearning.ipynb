{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 : Tools for Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD \n",
    "$W = W - n * dL/dw$\n",
    "\n",
    "$dL / dW  손실함수의 기울기$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lf = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD의 단점\n",
    "\n",
    "$f(x,y) = 1/20 * x^2 + y^2$ 을 생각해 보았을때 이는 종이를 양쪽 끝에서 잡아 처진 모양처럼 그려진다.\n",
    "\n",
    "이 함수에서 기울기는 (0, 0)에서 최소값이지만, 기울기의 대부분은 (x, 0)에 가깝게 향하고 있어 지그재그로 여러번 학습해야함\n",
    "\n",
    "지그재그로 심하게 굽어진 움직임을 보여주며, 비효율적으로 움직임을 확인할 수 있음\n",
    "\n",
    "SGD의 단점은 비등방성함수에서 탐색경로가 매우 비효율적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모멘텀 (Mometum) \n",
    "\n",
    "운동량을 뜻하는 단어로 물리와 관계가 있음.\n",
    "\n",
    "$v = av - n * dL / dW$\n",
    "\n",
    "$W = W + v\n",
    "\n",
    "av항은 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다. \n",
    "\n",
    "지면의 마찰이나 공기저항에 해당됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "학습률의 값에 따라 학습이 굉장히 달라지기 떄문에, 학습률을 정하는 기술로 학습률감소가 있다.\n",
    "\n",
    "처음에 크게 학습하다가 조금씩 작게 학습한다는 이야기로 실제 신경망 학습에서 자주 사용된다.\n",
    "\n",
    "학습률을 일괄적으로 낮추는 방법에서 더욱 발전시킨것이 AdaGrad이다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

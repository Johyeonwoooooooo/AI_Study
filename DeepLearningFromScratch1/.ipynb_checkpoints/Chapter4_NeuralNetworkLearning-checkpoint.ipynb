{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 : Neural Network Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손실 함수 loss function\n",
    "\n",
    "## 오차 제곱합\n",
    "\n",
    "$ {1 \\over 2}  \\displaystyle\\sum_{y=y0}^{yn}{(y - t) ^ 2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교차 엔트로피 오차\n",
    "\n",
    "$ E = - \\displaystyle\\sum_{y=y0}^{yn}{tlogy}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "# delta를 더하는 이유? -> y = 0 일 경우엔 log에 들어가면 에러가 난다. 이를 방지하기 위해 매우 작은 수를 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.11809565095832"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.0, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.6]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch learning\n",
    "\n",
    "mini batch는 왜 사용할까?\n",
    "\n",
    "-> 훈련 데이터가 많으면 많을 수록 iteration의 시간이 오래 걸림, loss function의 계산에도 부하가 옴\n",
    "\n",
    "-> 데이터의 일부를 추려 근사치로 이용하도록 함\n",
    "\n",
    "장점\n",
    "\n",
    "빠른 시간 내에 적당히 정확한 경로를 찾아낼 수 있음\n",
    "\n",
    "단점\n",
    "\n",
    "데이터 전체의 경향을 파악하지 않아 정확하지 않음\n",
    "\n",
    "**적당히 빠른 길을 찾기 위해 사용됨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "x_train.shape\n",
    "t_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "\n",
    "print(x_batch.shape[0])\n",
    "print(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size) # flatten answer \n",
    "        y = y.reshape(1, y.size) # flatten output \n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y+1e-7)) / batch_size # entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nt -> 정답이고 batch_size -> y 갯수로 설정함\\nnp.arange(batch_size)로 0~9 까지 생성시킴\\nt -> 정답 index 결국 y[0~9 , 정답 num]의 값을 가져와서 log 하는거랑 같음\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size) # flatten answer \n",
    "        y = y.reshape(1, y.size) # flatten output\n",
    "\n",
    "    batch_size = y.shape[0] \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "'''\n",
    "t -> 정답이고 batch_size -> y 갯수로 설정함\n",
    "np.arange(batch_size)로 0~9 까지 생성시킴\n",
    "t -> 정답 index 결국 y[0~9 , 정답 num]의 값을 가져와서 log 하는거랑 같음\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function을 propagation의 지표로 사용하는 이유\n",
    "\n",
    "정확도를 기준으로 학습시키면 좋을 것 같지만, 정확도로 학습시키면 시킬수록 미분값이 0이 됨.\n",
    "\n",
    "즉 학습이 이루어지지 않음\n",
    "\n",
    "그렇기 때문에 손실함수를 사용함\n",
    "\n",
    "ex) chapter3 에서의 계단함수와 시그모이드 함수 비교\n",
    "    계단함수는 대부분의 구간에서 미분값이 0, 시그모이드 함수는 미분값이 거의 항상 변함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def numerical_diff_badcase(f, x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 미분 식이 Bad case인 이유\n",
    "\n",
    "1. np.float32(h)를 했을 때 0이 나옴. 즉, 너무 작은 값을 사용하려하다 보니\n",
    "\n",
    "    오히려 0으로 취급되어서 오류를 발생시킬 수 도 있음 (반올림 오차 rounding error)\n",
    "\n",
    "2. 미분은 x위치의 함수의 기울기에 해당함. \n",
    "   \n",
    "   위 함수의 구현은 (x+h)와 x사이의 기울기에 해당하기 때문에 진정한 미분과는 거리가 있음.\n",
    "   \n",
    "   즉, h를 0으로 무한하게 좁히는것이 불가능하기 때문에 생기는 한계이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 중간차분법이 좋은 이유는 증명이 길기 때문에 아래 작성한 벨로그를 참고\n",
    "\n",
    "https://velog.io/@hyunwoo02031/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EC%88%98%EC%B9%98%EB%AF%B8%EB%B6%84-%EC%B0%A8%EB%B6%84%EB%B2%95%EC%97%90-%EB%94%B0%EB%A5%B8-%EC%98%A4%EC%B0%A8\n",
    "\n",
    "h = 1e-4 로 둔 이유는 casting 시 0.0이 되지 않으면서 가장 적은 오차를 내는 h가 1e-4로 알려져 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01 * x ** 2 + 0.1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1999999999990898\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff_badcase(function_1, 5))\n",
    "print(numerical_diff(function_1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 편미분\n",
    "\n",
    "$f(x0, x1) = (x0)^2 + (x1)^2$\n",
    "\n",
    "x0 = 3, x1 = 4 일 때 x0 편미분, x1 편미분을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0 * x0 + 4 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3 ** 2 + x1 * x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n",
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_tmp1, 3))\n",
    "print(numerical_diff(function_tmp2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "numerical_gradient(function_2, np.array([3.0, 0.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사법\n",
    " : 기계학습을 최적화하기 위해 사용함.\n",
    " \n",
    "   현위치에서 기울어진 방향으로 조금씩 이동시키며 기울기를 측정하여 손실함수의 값을 줄어들게 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.999994  3.999992]\n"
     ]
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=10))\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=0.00000001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.68603858 -0.74684455  0.04812626]\n",
      " [ 1.92528321  0.39255156 -0.59877181]]\n",
      "[ 1.32113173 -0.09481033 -0.51001887]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1697097660743663"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "np.argmax(p)\n",
    "\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.42767722  0.10379592 -0.53147314]\n",
      " [ 0.64151583  0.15569388 -0.79720971]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 알고리즘 구현하기\n",
    "\n",
    "1 단계 : 미니배치\n",
    "2 단계 : 기울기 산출\n",
    "3 단계 : 매개변수 갱신\n",
    "4 단계 : 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(a1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09888903 0.09945098 0.10008727 0.10083313 0.10273131 0.0999298\n",
      "  0.10045266 0.10067978 0.09836384 0.09858219]\n",
      " [0.09954753 0.09758287 0.10053931 0.10145842 0.10168152 0.10160273\n",
      "  0.09908101 0.09883692 0.09932252 0.10034717]\n",
      " [0.09991062 0.09975491 0.09985943 0.10098831 0.10026037 0.1008677\n",
      "  0.10114654 0.09995366 0.09651489 0.10074356]\n",
      " [0.09956085 0.0990841  0.10057185 0.10191001 0.10018519 0.10047275\n",
      "  0.10054473 0.09947673 0.09810312 0.10009069]\n",
      " [0.09944471 0.09996363 0.09955346 0.10067643 0.10228634 0.09979889\n",
      "  0.10149534 0.1001957  0.09663116 0.09995434]\n",
      " [0.10079502 0.09786211 0.10222733 0.10122754 0.10128673 0.10157317\n",
      "  0.09994093 0.0991916  0.09607642 0.09981915]\n",
      " [0.09972279 0.09903933 0.10042982 0.10129014 0.10135182 0.10035109\n",
      "  0.10098259 0.09961009 0.09619218 0.10103016]\n",
      " [0.0994585  0.09904341 0.0995063  0.10144985 0.10192529 0.10030676\n",
      "  0.09996528 0.10099186 0.09737689 0.09997586]\n",
      " [0.10013768 0.09905137 0.09898871 0.10019971 0.10243521 0.10058467\n",
      "  0.10035079 0.10000307 0.09862422 0.09962458]\n",
      " [0.09919587 0.09901175 0.10035861 0.09968401 0.10163813 0.09993736\n",
      "  0.10083457 0.10012395 0.09887247 0.10034328]\n",
      " [0.09889492 0.0979395  0.0996309  0.1017501  0.10110407 0.10151376\n",
      "  0.10034375 0.10080858 0.09803949 0.09997493]\n",
      " [0.09849042 0.09923086 0.10085936 0.10158236 0.10107235 0.10090912\n",
      "  0.10033291 0.10078238 0.09739557 0.09934468]\n",
      " [0.09975001 0.09997164 0.09954355 0.10031836 0.10062785 0.1001335\n",
      "  0.10154765 0.10086653 0.09714966 0.10009125]\n",
      " [0.0990328  0.09844834 0.0986382  0.10107361 0.10202674 0.10051009\n",
      "  0.10028724 0.10039915 0.09933821 0.10024562]\n",
      " [0.09953296 0.09991211 0.10038555 0.10052927 0.10065814 0.10106311\n",
      "  0.1005553  0.09921394 0.09804393 0.1001057 ]\n",
      " [0.10034099 0.09969045 0.10021602 0.10106596 0.09973229 0.09976595\n",
      "  0.10159638 0.10167313 0.09594308 0.09997576]\n",
      " [0.09948894 0.09902642 0.09927378 0.10228125 0.10070594 0.10127898\n",
      "  0.10054755 0.10092593 0.09755321 0.09891799]\n",
      " [0.09998846 0.09932229 0.09986461 0.10030443 0.10212575 0.09950416\n",
      "  0.10061163 0.099411   0.09814739 0.10072027]\n",
      " [0.10108347 0.09788247 0.10032076 0.10100733 0.09998343 0.10223441\n",
      "  0.10056551 0.09986478 0.09700012 0.10005773]\n",
      " [0.09947511 0.09941761 0.10066201 0.09918445 0.10153561 0.1019304\n",
      "  0.10073222 0.09939273 0.09865916 0.09901068]\n",
      " [0.09922795 0.09904821 0.1024159  0.10057732 0.10062221 0.09965669\n",
      "  0.10133161 0.10041309 0.09646492 0.1002421 ]\n",
      " [0.09948169 0.09993796 0.09989105 0.10146277 0.10070791 0.10079054\n",
      "  0.10109515 0.09983009 0.09800334 0.09879951]\n",
      " [0.10065902 0.1009094  0.09897726 0.10147819 0.10005486 0.1012111\n",
      "  0.10041648 0.09933379 0.09650793 0.10045198]\n",
      " [0.09991501 0.10061865 0.0976097  0.10163045 0.10156843 0.0989852\n",
      "  0.10022246 0.10082024 0.0987034  0.09992647]\n",
      " [0.10036507 0.09879355 0.10062105 0.09975231 0.10214265 0.1006625\n",
      "  0.1006829  0.09993032 0.0980667  0.09898295]\n",
      " [0.09944812 0.09936565 0.10063646 0.1025351  0.10141185 0.10020792\n",
      "  0.09904317 0.0992049  0.09729011 0.10085672]\n",
      " [0.10084592 0.09905746 0.10162185 0.10094327 0.10034639 0.09979898\n",
      "  0.10176937 0.09932209 0.09722108 0.09907359]\n",
      " [0.09957676 0.09961687 0.10111897 0.0998029  0.10089883 0.1004236\n",
      "  0.10125738 0.09946505 0.09808802 0.09975162]\n",
      " [0.09947192 0.09911697 0.10051758 0.0996132  0.10175742 0.09966793\n",
      "  0.10023247 0.10181069 0.09797614 0.09983569]\n",
      " [0.10028865 0.09913158 0.10066431 0.09886085 0.10136869 0.10003205\n",
      "  0.10043199 0.09966725 0.09846099 0.10109366]\n",
      " [0.1001208  0.09971595 0.1009683  0.10121027 0.10158333 0.09840963\n",
      "  0.1000728  0.10045985 0.09745735 0.1000017 ]\n",
      " [0.10064059 0.09940261 0.10000087 0.10015205 0.10037061 0.10147074\n",
      "  0.10100352 0.09993906 0.09653735 0.1004826 ]\n",
      " [0.09960842 0.10045313 0.09963959 0.09956685 0.10213705 0.10066518\n",
      "  0.10003936 0.09979348 0.09815413 0.09994281]\n",
      " [0.10080038 0.09886739 0.10042092 0.10092472 0.09998361 0.10099701\n",
      "  0.10043591 0.10072138 0.09714055 0.09970813]\n",
      " [0.09981132 0.09936419 0.10038187 0.10009328 0.10044471 0.10190857\n",
      "  0.1002811  0.10007679 0.09747352 0.10016464]\n",
      " [0.09933524 0.09972047 0.09863342 0.10035258 0.10180285 0.10023058\n",
      "  0.10037688 0.10106202 0.09871823 0.09976772]\n",
      " [0.09964726 0.1001032  0.10031034 0.1021787  0.10164438 0.10016826\n",
      "  0.10082839 0.09923641 0.09639189 0.09949117]\n",
      " [0.0997729  0.09819963 0.09907508 0.10049059 0.10179391 0.09970396\n",
      "  0.10242844 0.10124823 0.0973073  0.09997995]\n",
      " [0.10093545 0.09795022 0.09954961 0.10046073 0.10169726 0.10010989\n",
      "  0.10109777 0.10090837 0.09694815 0.10034255]\n",
      " [0.09974796 0.09952048 0.10072586 0.09895127 0.10127829 0.10024272\n",
      "  0.09977682 0.09938271 0.09987822 0.10049566]\n",
      " [0.10057814 0.09895462 0.09862827 0.10198933 0.10050205 0.10016457\n",
      "  0.10043906 0.10103683 0.09756639 0.10014073]\n",
      " [0.10062897 0.09750129 0.10049154 0.10184896 0.10134244 0.0999156\n",
      "  0.10085031 0.09988155 0.09685517 0.10068418]\n",
      " [0.09985112 0.1001422  0.10001128 0.10056694 0.10160617 0.10035956\n",
      "  0.10049153 0.10019734 0.09658523 0.10018864]\n",
      " [0.10187241 0.09916155 0.09882059 0.10085882 0.10122898 0.10067703\n",
      "  0.10020026 0.09908658 0.09728517 0.1008086 ]\n",
      " [0.09859477 0.0987793  0.10015301 0.10068634 0.10250702 0.10003099\n",
      "  0.10037424 0.10065353 0.09819824 0.10002256]\n",
      " [0.10078764 0.0978742  0.10011854 0.10136463 0.10053764 0.10185217\n",
      "  0.10077043 0.1001708  0.09680913 0.09971481]\n",
      " [0.09937253 0.09907463 0.09842556 0.10145188 0.10141182 0.10027302\n",
      "  0.10079721 0.10053543 0.09916861 0.09948933]\n",
      " [0.10075884 0.09971228 0.0994584  0.1001673  0.10196865 0.10037833\n",
      "  0.10048297 0.10042502 0.09775966 0.09888855]\n",
      " [0.10076526 0.09943654 0.09931029 0.10189942 0.09912873 0.09935801\n",
      "  0.10018863 0.10143597 0.0980962  0.10038096]\n",
      " [0.10034921 0.10051266 0.09955365 0.10147661 0.10206632 0.09834281\n",
      "  0.10047172 0.09951328 0.09788652 0.09982722]\n",
      " [0.1002117  0.09820222 0.09958989 0.1010703  0.1004925  0.10105742\n",
      "  0.10085032 0.1004143  0.0976533  0.10045805]\n",
      " [0.10040059 0.09907996 0.10062917 0.10233216 0.10099836 0.10168048\n",
      "  0.10013071 0.09832958 0.09674332 0.09967567]\n",
      " [0.10043623 0.09923724 0.09929128 0.10190152 0.10003894 0.09961096\n",
      "  0.10002994 0.10070488 0.09808125 0.10066776]\n",
      " [0.10040046 0.09971516 0.09919242 0.10127795 0.10106796 0.10001866\n",
      "  0.09938516 0.10101795 0.09847256 0.09945173]\n",
      " [0.10047696 0.09827442 0.10027899 0.10097916 0.10115107 0.10015019\n",
      "  0.09969647 0.10121493 0.09729007 0.10048773]\n",
      " [0.09937715 0.09874535 0.10013537 0.1001595  0.10105737 0.10189532\n",
      "  0.10093101 0.09998094 0.09742566 0.10029236]\n",
      " [0.0997305  0.09929083 0.10168899 0.10130624 0.10049218 0.09924995\n",
      "  0.10016626 0.10050572 0.09733987 0.10022947]\n",
      " [0.09945995 0.09997038 0.10016122 0.10270169 0.10022536 0.09831238\n",
      "  0.10015218 0.10059058 0.09852778 0.09989848]\n",
      " [0.0995778  0.09887642 0.09947105 0.10119071 0.10082416 0.10122602\n",
      "  0.10036834 0.10064202 0.09756723 0.10025624]\n",
      " [0.10067734 0.09900735 0.09922289 0.10017877 0.10238456 0.10063228\n",
      "  0.1000853  0.10005125 0.0977903  0.09996996]\n",
      " [0.09957954 0.10016471 0.10039271 0.10012471 0.10137613 0.09976072\n",
      "  0.10031119 0.09978824 0.09820422 0.10029784]\n",
      " [0.09975361 0.09891627 0.10055486 0.10055843 0.10127116 0.10004063\n",
      "  0.10047957 0.10067276 0.09731382 0.1004389 ]\n",
      " [0.10072208 0.09945241 0.10030485 0.10064358 0.10152806 0.10100979\n",
      "  0.10068022 0.09965947 0.09680476 0.09919478]\n",
      " [0.09982999 0.09813167 0.10016073 0.10211735 0.10031438 0.10062485\n",
      "  0.10085727 0.09982091 0.09907285 0.09907001]\n",
      " [0.09963707 0.09994973 0.09988361 0.09995066 0.10150062 0.10092467\n",
      "  0.10093815 0.10145343 0.09644819 0.09931387]\n",
      " [0.09998678 0.09836375 0.10002124 0.0995375  0.10241781 0.10085387\n",
      "  0.1017135  0.1000665  0.09709986 0.09993919]\n",
      " [0.09965523 0.09958123 0.10114039 0.10189686 0.10095059 0.09904783\n",
      "  0.09976246 0.10081682 0.09713561 0.10001299]\n",
      " [0.10100793 0.09873283 0.09865269 0.10187403 0.10136327 0.10024345\n",
      "  0.10136587 0.10008159 0.09782671 0.09885163]\n",
      " [0.09816114 0.09891265 0.10100738 0.10081338 0.10093418 0.10034916\n",
      "  0.10037396 0.10132903 0.09747747 0.10064164]\n",
      " [0.0994598  0.10081534 0.10004063 0.10052603 0.10142663 0.09888776\n",
      "  0.10036637 0.10130308 0.09760235 0.09957202]\n",
      " [0.09979558 0.09924181 0.09997222 0.09944494 0.10244975 0.10063499\n",
      "  0.101081   0.10114627 0.09641456 0.09981888]\n",
      " [0.10049742 0.09956419 0.0990418  0.10133958 0.10043508 0.10097614\n",
      "  0.10044571 0.10167687 0.0964779  0.09954532]\n",
      " [0.0993648  0.09872834 0.10032123 0.1013557  0.10207439 0.10025325\n",
      "  0.10090767 0.10010214 0.09738431 0.09950819]\n",
      " [0.10052409 0.09955477 0.0996877  0.10076479 0.10063879 0.10004786\n",
      "  0.10117919 0.09922298 0.09829204 0.1000878 ]\n",
      " [0.09915007 0.09865949 0.098825   0.10063621 0.10099674 0.09965616\n",
      "  0.10062394 0.10135022 0.09951749 0.10058468]\n",
      " [0.1004618  0.09907873 0.10120754 0.10098476 0.10248109 0.09952295\n",
      "  0.10032312 0.09920856 0.09638853 0.10034293]\n",
      " [0.10090392 0.09984974 0.09932012 0.10113778 0.09949427 0.10015397\n",
      "  0.1009017  0.09947606 0.09821607 0.10054639]\n",
      " [0.0994046  0.09799381 0.10056757 0.10023758 0.10210611 0.10045804\n",
      "  0.10069041 0.09970021 0.09840207 0.1004396 ]\n",
      " [0.09955448 0.09963129 0.10025512 0.10039391 0.10141827 0.10122762\n",
      "  0.10084305 0.09988304 0.09685286 0.09994035]\n",
      " [0.09908473 0.10031132 0.10071602 0.09965129 0.09996415 0.09867181\n",
      "  0.10122845 0.10112449 0.0988664  0.10038134]\n",
      " [0.09960948 0.10041399 0.10158617 0.10126869 0.10008271 0.10133704\n",
      "  0.10133337 0.098646   0.09668079 0.09904176]\n",
      " [0.09901148 0.09995039 0.09945942 0.09977238 0.10242171 0.10036193\n",
      "  0.09993405 0.1008056  0.09883623 0.09944679]\n",
      " [0.10049494 0.09799964 0.10115146 0.10177136 0.10037931 0.10085443\n",
      "  0.1005311  0.09959366 0.09749384 0.09973026]\n",
      " [0.09953114 0.09876792 0.10147123 0.10031682 0.10185252 0.10142272\n",
      "  0.09884578 0.10036327 0.09760085 0.09982775]\n",
      " [0.09914034 0.10091765 0.09972876 0.10068944 0.10162672 0.09956291\n",
      "  0.100659   0.10143285 0.096316   0.09992632]\n",
      " [0.09879134 0.09807964 0.1000522  0.10110163 0.10165282 0.10078061\n",
      "  0.10062817 0.09997927 0.09796343 0.10097089]\n",
      " [0.09937607 0.10055552 0.09996057 0.09893761 0.10218822 0.09913644\n",
      "  0.10141379 0.10092372 0.0976956  0.09981244]\n",
      " [0.10043542 0.09853581 0.09874945 0.10054848 0.10100589 0.10065858\n",
      "  0.10161385 0.10010605 0.0978729  0.10047357]\n",
      " [0.10065671 0.09933985 0.10104761 0.09993109 0.10126137 0.10036347\n",
      "  0.10183028 0.10002054 0.09581468 0.09973439]\n",
      " [0.09936102 0.09889819 0.09974043 0.10047232 0.10077217 0.10047709\n",
      "  0.10175998 0.09906705 0.09821431 0.10123744]\n",
      " [0.10004484 0.09957234 0.09974104 0.09979043 0.10073864 0.10022997\n",
      "  0.10027369 0.09969986 0.09885561 0.10105358]\n",
      " [0.09941219 0.09897785 0.09996112 0.10054775 0.10198914 0.10029312\n",
      "  0.10056387 0.10073467 0.09797926 0.09954103]\n",
      " [0.09896186 0.09873813 0.10075251 0.100639   0.10164228 0.10076106\n",
      "  0.10075636 0.09983282 0.09817552 0.09974045]\n",
      " [0.09931534 0.09880399 0.10073586 0.10025563 0.10249434 0.10087986\n",
      "  0.10148173 0.09901753 0.09765967 0.09935606]\n",
      " [0.09837796 0.09879386 0.09933316 0.1009125  0.10224717 0.10108737\n",
      "  0.09984014 0.10137957 0.09860592 0.09942235]\n",
      " [0.09967118 0.09992292 0.10006853 0.10002813 0.10272738 0.10126057\n",
      "  0.09981886 0.09974796 0.09740784 0.09934665]\n",
      " [0.09828685 0.09935532 0.10162399 0.10074264 0.1036937  0.09848857\n",
      "  0.10100505 0.0991597  0.09738645 0.10025774]\n",
      " [0.09935837 0.09912884 0.10047041 0.10201694 0.10099784 0.09971167\n",
      "  0.09940784 0.10127845 0.09766664 0.09996301]\n",
      " [0.0999902  0.09846294 0.10038262 0.10154806 0.10211754 0.10119\n",
      "  0.10015961 0.09923906 0.0978264  0.09908357]\n",
      " [0.09957591 0.09949448 0.10143804 0.10076177 0.10054669 0.09915612\n",
      "  0.10035828 0.10194281 0.09723655 0.09948936]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[0;32m      6\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(grads)\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     40\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     41\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 42\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     44\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\ML\\AI_Study\\DeepLearningFromScratch1\\common\\gradient.py:43\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "print(y)\n",
    "\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01014804 -0.0077111  -0.00856143 ... -0.01385626  0.01927759\n",
      "   0.00814292]\n",
      " [-0.01000537 -0.01716414 -0.00433169 ... -0.00358031  0.00023227\n",
      "   0.01318744]\n",
      " [ 0.00373562 -0.00045668  0.01420791 ... -0.01153021 -0.01824721\n",
      "  -0.00390782]\n",
      " ...\n",
      " [ 0.00489054 -0.00859366  0.00050149 ...  0.0137628  -0.00199017\n",
      "  -0.00376409]\n",
      " [-0.00746639 -0.00618043 -0.00269601 ... -0.0036553   0.00325094\n",
      "   0.0053089 ]\n",
      " [ 0.0057966  -0.00635269  0.00753682 ... -0.00884829  0.01174737\n",
      "  -0.00188274]]\n",
      "[[-6.98283963e-03  1.06252625e-02  5.75485744e-03  6.71660739e-03\n",
      "  -6.05765331e-04  1.26461117e-02 -1.00469046e-02 -8.87589364e-04\n",
      "   1.09178368e-02 -2.53898693e-03]\n",
      " [-1.44141215e-02  2.11304167e-03 -7.21729553e-03  1.82972764e-02\n",
      "   1.26364938e-02 -1.90806798e-02  6.21757782e-03  8.91331515e-03\n",
      "   6.20451043e-04  1.08186681e-02]\n",
      " [ 9.44849216e-03 -8.89245258e-03 -9.42959540e-03 -2.53965778e-05\n",
      "  -1.20094183e-02 -6.63437922e-03  5.15388224e-03  2.56284889e-03\n",
      "   1.52379487e-02 -9.75347301e-03]\n",
      " [ 1.80956010e-02  5.73290756e-03 -3.85490368e-03  1.22414718e-02\n",
      "  -2.58262550e-03  1.50469426e-02 -1.87328310e-03  1.82761429e-03\n",
      "   4.35825067e-03  3.96538363e-03]\n",
      " [-1.29555147e-02  7.46840215e-03 -4.18631846e-03 -2.69196667e-02\n",
      "   1.95412487e-02 -1.83176597e-02  1.00642186e-02  7.37035214e-03\n",
      "   4.13993891e-03  3.00514418e-03]\n",
      " [-1.47372701e-02 -3.81964128e-03  1.22234578e-02  9.87949203e-03\n",
      "   4.28800870e-03 -7.35804024e-03 -3.07997163e-03 -1.09510788e-02\n",
      "   1.61343689e-03  8.54998360e-03]\n",
      " [ 6.35441169e-04 -8.90762393e-03 -2.27456945e-03  3.22845780e-03\n",
      "  -1.29283076e-05  1.07374344e-03  2.13867262e-02  3.69675637e-03\n",
      "   9.27735597e-03 -6.51182853e-03]\n",
      " [ 7.28486257e-03  3.20632460e-03 -1.08926809e-02 -3.11211317e-03\n",
      "  -2.45504462e-03 -6.62634831e-03 -1.62847740e-02  6.84145557e-03\n",
      "  -3.34840428e-02 -1.74307281e-02]\n",
      " [-4.02964336e-03 -6.92465323e-03  1.87611319e-02  6.13847877e-03\n",
      "  -1.34320636e-02  5.03340971e-03  3.39059465e-03  7.17250782e-03\n",
      "   8.13966369e-03 -3.97784265e-03]\n",
      " [-1.37724736e-03  1.16433702e-03  7.69403697e-03 -1.54294203e-03\n",
      "   2.22427894e-05 -1.72827518e-03 -5.77906624e-03  1.17673324e-02\n",
      "  -5.24560591e-03  1.62298044e-02]\n",
      " [-1.07836751e-02  2.46009522e-03  1.02751020e-02 -7.45790371e-03\n",
      "  -1.06184132e-02 -4.07323790e-03 -1.38597965e-03  4.65002331e-03\n",
      "   1.35618281e-02  2.19374927e-03]\n",
      " [ 4.45182606e-03 -1.09923606e-02 -7.27204244e-03 -5.15451600e-03\n",
      "   3.50509668e-02  4.18976213e-03  2.52630008e-03 -1.24082479e-02\n",
      "  -6.91098466e-03  5.16758348e-03]\n",
      " [ 1.41385550e-02  4.49787470e-03  4.78583914e-03  2.07026440e-03\n",
      "   1.83671387e-02  1.09913552e-02  1.32639870e-02  2.53928207e-03\n",
      "   1.59934249e-02 -8.37307603e-03]\n",
      " [-9.42920554e-03 -1.26710846e-02  2.50412614e-03 -1.45442591e-03\n",
      "   4.42094592e-03 -1.37806214e-02 -2.75102930e-03 -1.76140921e-02\n",
      "  -1.18132569e-02 -7.14290278e-03]\n",
      " [ 9.27535006e-03 -1.03489811e-02  1.64532790e-03  2.42466608e-03\n",
      "  -1.53714956e-02 -6.48715378e-03 -1.31867248e-02 -1.69750189e-02\n",
      "  -9.21019617e-03  4.31211115e-03]\n",
      " [ 2.10413288e-02 -2.44981361e-03  1.77236453e-03 -4.96283558e-03\n",
      "  -8.93689277e-03  1.44063815e-03 -1.21858732e-03  9.54144281e-03\n",
      "  -1.24243401e-03 -2.73845607e-03]\n",
      " [-7.99286560e-03 -7.51793172e-03  5.03072606e-03 -8.78375359e-03\n",
      "   6.81921095e-03 -1.75385541e-02 -4.93722779e-03  9.81991839e-03\n",
      "  -4.43781703e-03  1.48423778e-02]\n",
      " [-1.47291240e-02 -2.91259005e-02  1.48314397e-02  1.01374974e-02\n",
      "  -1.77220207e-02 -9.22145696e-03  7.33909364e-03 -1.25947613e-02\n",
      "  -1.98216279e-02 -9.80363280e-03]\n",
      " [ 1.23140621e-02 -6.96591154e-03 -5.93296573e-03 -1.04286938e-02\n",
      "  -1.66189964e-02 -4.21708664e-03 -4.94594961e-03 -2.55429062e-03\n",
      "   7.61832026e-03  1.55958317e-02]\n",
      " [-6.21405904e-03 -2.12161314e-02 -3.79328264e-03 -3.02242257e-04\n",
      "   5.69693810e-03  6.41946493e-03  2.76459260e-03  2.67026139e-03\n",
      "   2.67147041e-03 -6.30999587e-03]\n",
      " [-1.17668557e-03 -1.28981294e-03  2.26103818e-03 -5.28334338e-03\n",
      "   3.84669953e-03 -4.66518253e-03 -3.04536654e-02  3.30508053e-03\n",
      "   4.65806398e-03 -1.05436793e-02]\n",
      " [ 4.40115935e-03 -9.38557403e-03 -6.47113568e-04  6.80862695e-03\n",
      "   2.19102776e-03 -1.67672107e-02 -2.31977113e-02 -8.83710575e-03\n",
      "   7.43505224e-04  4.12657573e-03]\n",
      " [ 6.77346656e-03 -1.09696134e-03  9.91047855e-03 -4.63360532e-03\n",
      "  -7.17168800e-03  1.54119415e-02  9.65262635e-03 -6.67709051e-03\n",
      "  -4.51505038e-03  2.40925534e-02]\n",
      " [ 2.69370310e-05  5.23136625e-03  1.21799964e-02  1.76526694e-03\n",
      "   2.13079599e-03 -1.09686609e-03  4.64675535e-03  8.04689461e-03\n",
      "   6.66343558e-03  2.39488088e-02]\n",
      " [ 4.22886048e-03  4.15592743e-03 -1.76982344e-02 -2.84739206e-02\n",
      "   1.44832086e-03  1.50784754e-02 -9.20534543e-03 -1.42117338e-02\n",
      "   9.52260833e-04  8.59628307e-03]\n",
      " [ 8.48972753e-03  6.10240967e-03  2.12554088e-03  2.27350756e-03\n",
      "  -1.54932364e-02 -7.16892950e-03 -5.39585112e-03 -2.22148672e-03\n",
      "  -3.56721880e-03  1.05144584e-02]\n",
      " [ 1.37863690e-03  8.87717076e-03 -8.70310347e-03 -1.14721805e-02\n",
      "   5.93375269e-03  1.78180521e-02  5.43421611e-03  9.61573297e-03\n",
      "  -3.72495444e-03 -8.78239507e-03]\n",
      " [ 4.48266275e-03 -6.07757842e-03  1.46523148e-02  5.94731021e-03\n",
      "  -7.64313840e-03  1.61776438e-02  1.57770167e-02 -1.04247743e-02\n",
      "  -7.00217568e-03 -7.42888412e-03]\n",
      " [ 7.32019590e-03 -1.32514630e-02 -6.84974699e-03 -4.19648366e-03\n",
      "   9.58005304e-03 -7.63917253e-03 -5.39983398e-03 -1.26429414e-02\n",
      "   1.38491794e-02 -2.89497617e-03]\n",
      " [-7.49268736e-03 -8.41922517e-03  2.32694183e-02 -4.98266105e-03\n",
      "  -6.89752935e-03 -8.62200021e-03  8.39539113e-03  8.69477094e-03\n",
      "   1.16250218e-02  3.07919149e-03]\n",
      " [ 8.57574435e-03  1.90074322e-02 -2.18771899e-03 -2.53364847e-03\n",
      "   2.52366827e-03  1.89967309e-02  3.01868715e-03  4.22196103e-03\n",
      "  -3.15235255e-03 -2.71703591e-03]\n",
      " [ 1.94736298e-02  1.80681570e-02 -2.77148666e-03  1.97734241e-04\n",
      "   3.90884454e-03  4.75406861e-03  9.78264082e-03  1.18363483e-02\n",
      "  -3.04834951e-03  4.11305508e-03]\n",
      " [-6.11324128e-03 -6.80132807e-03 -2.12307866e-03 -8.26296125e-03\n",
      "  -2.10361384e-03 -9.97986204e-04  3.78373959e-03  1.26355776e-02\n",
      "   1.44832430e-02  2.77548565e-03]\n",
      " [ 6.88081869e-03 -1.79964600e-02  1.52004541e-04 -2.58295182e-03\n",
      "  -7.28279967e-03 -6.80065395e-03  1.35190605e-02  8.46267629e-03\n",
      "  -1.85559094e-02  3.98102236e-03]\n",
      " [ 8.92743874e-03  3.69424054e-03 -3.12721689e-03 -1.90096547e-02\n",
      "  -2.76139999e-03 -1.26637405e-02  1.09771105e-02 -7.36954435e-03\n",
      "  -2.59097940e-02  6.88636690e-05]\n",
      " [ 8.99272129e-03  6.54235899e-03  3.09620270e-03 -2.20499432e-02\n",
      "  -1.88963917e-02 -9.84973134e-03 -5.56008739e-03 -9.43110875e-03\n",
      "   6.07356605e-03  7.67720564e-03]\n",
      " [ 9.52487163e-03  2.36652044e-02  9.36240462e-03 -7.67047863e-03\n",
      "  -1.33737787e-02  8.26791719e-03 -8.43499118e-03 -7.86592981e-03\n",
      "   5.10570567e-03 -6.46276578e-03]\n",
      " [ 5.73062680e-03 -6.55092045e-03  1.98632404e-03  8.31113901e-03\n",
      "  -1.42390594e-02  9.96012576e-03  1.00038664e-02 -3.62567916e-03\n",
      "   1.22433086e-02 -1.67535062e-02]\n",
      " [-1.36958797e-02 -8.39455265e-03 -3.78595767e-03  7.81135493e-03\n",
      "   7.23732011e-03  3.79421742e-03 -1.06762368e-02 -5.41723616e-03\n",
      "  -1.05406108e-02  6.66449878e-03]\n",
      " [ 1.82142775e-02 -1.01180485e-02 -1.21958584e-03  1.07168656e-02\n",
      "  -8.95834611e-03  1.76834144e-02  4.93175955e-03  2.28799873e-03\n",
      "   8.80334215e-03 -7.84520878e-03]\n",
      " [ 1.40201345e-03  9.88137077e-03  8.43059451e-03 -2.60908123e-03\n",
      "   2.28431685e-03  1.93979094e-03 -1.55488609e-02 -4.60204700e-04\n",
      "   5.51345962e-04  1.47966396e-03]\n",
      " [-1.04910799e-02 -6.89725784e-03  8.66885006e-03  1.65854393e-03\n",
      "  -1.38668623e-02 -1.66230352e-02  1.59084519e-02  1.73837539e-04\n",
      "  -1.71504552e-02  8.20177629e-03]\n",
      " [ 2.04859599e-03  4.95197582e-04 -2.98711737e-03 -1.11313690e-02\n",
      "   5.80708985e-03  1.66706024e-03 -2.08530298e-02 -1.19467922e-02\n",
      "  -9.11254995e-03 -1.53044314e-02]\n",
      " [-1.84993071e-02 -1.93820754e-02 -1.18936986e-02  5.02785004e-03\n",
      "  -1.06333413e-02 -6.48738275e-03  2.74703647e-02 -1.44207063e-02\n",
      "   7.45206748e-03 -3.66354760e-03]\n",
      " [-1.28987450e-02  1.49879194e-03 -8.40900554e-04 -7.41438007e-03\n",
      "   2.40079439e-03  8.17660411e-03 -5.92488710e-03  6.92049779e-03\n",
      "   2.63426331e-03  2.81505795e-02]\n",
      " [-8.18625587e-03  1.52927194e-02  4.41334115e-03 -5.62246503e-03\n",
      "  -1.31810164e-02  1.08244580e-02  7.61647117e-03 -4.08196201e-03\n",
      "   8.69530451e-03  2.99201572e-03]\n",
      " [-9.69902015e-03  5.80384028e-03 -1.29698645e-02 -6.44519955e-03\n",
      "   6.72888201e-03  9.50194982e-03 -1.53235473e-02  1.51761003e-02\n",
      "  -4.93524562e-03 -2.48539957e-02]\n",
      " [ 1.93433047e-04 -4.51515443e-03  8.60752317e-03 -4.41299288e-03\n",
      "   5.38400821e-03  1.12555117e-02  1.45009678e-02 -1.26160936e-02\n",
      "  -2.83274447e-03 -1.27235795e-02]\n",
      " [-4.65742698e-03  1.32555513e-02  1.25014039e-02  2.11962838e-03\n",
      "   4.67796252e-03  1.41243752e-02 -1.25854319e-02  2.69617819e-03\n",
      "   1.90017944e-02  1.24510436e-02]\n",
      " [-2.45424189e-03  1.36699553e-02  2.69442931e-03  9.24175110e-03\n",
      "   2.67745243e-02 -4.18098256e-03 -2.87996659e-03 -1.17017643e-03\n",
      "  -1.24503198e-03 -5.68540259e-03]]\n",
      "[[ 0.01014804 -0.0077111  -0.00856143 ... -0.01385626  0.01927759\n",
      "   0.00814292]\n",
      " [-0.01000537 -0.01716414 -0.00433169 ... -0.00358031  0.00023227\n",
      "   0.01318744]\n",
      " [ 0.00373562 -0.00045668  0.01420791 ... -0.01153021 -0.01824721\n",
      "  -0.00390782]\n",
      " ...\n",
      " [ 0.00489054 -0.00859366  0.00050149 ...  0.0137628  -0.00199017\n",
      "  -0.00376409]\n",
      " [-0.00746639 -0.00618043 -0.00269601 ... -0.0036553   0.00325094\n",
      "   0.0053089 ]\n",
      " [ 0.0057966  -0.00635269  0.00753682 ... -0.00884829  0.01174737\n",
      "  -0.00188274]]\n",
      "[[-6.93186070e-03  1.05222981e-02  5.78857859e-03  6.59482966e-03\n",
      "  -5.93557465e-04  1.25039401e-02 -9.97250063e-03 -9.46742145e-04\n",
      "   1.09660092e-02 -2.33240474e-03]\n",
      " [-1.43446751e-02  2.01349721e-03 -7.17583119e-03  1.81634473e-02\n",
      "   1.26453386e-02 -1.92401093e-02  6.26142872e-03  8.86634538e-03\n",
      "   6.63600855e-04  1.10516847e-02]\n",
      " [ 9.48179052e-03 -8.97143061e-03 -9.40052371e-03 -1.48129692e-04\n",
      "  -1.20000082e-02 -6.78624000e-03  5.21764140e-03  2.51574891e-03\n",
      "   1.52806376e-02 -9.53102932e-03]\n",
      " [ 1.81373102e-02  5.64564905e-03 -3.82360320e-03  1.21258292e-02\n",
      "  -2.58063012e-03  1.48940813e-02 -1.81650628e-03  1.76857211e-03\n",
      "   4.41531896e-03  4.19133806e-03]\n",
      " [-1.28934971e-02  7.36828946e-03 -4.14988769e-03 -2.70500721e-02\n",
      "   1.95594020e-02 -1.84727578e-02  1.01071185e-02  7.31306593e-03\n",
      "   4.18269846e-03  3.24578542e-03]\n",
      " [-1.47160831e-02 -3.88568699e-03  1.22667101e-02  9.74788214e-03\n",
      "   4.29904988e-03 -7.50266955e-03 -3.04332810e-03 -1.09766413e-02\n",
      "   1.64237450e-03  8.77676929e-03]\n",
      " [ 6.60773379e-04 -9.01080028e-03 -2.26861113e-03  3.09198830e-03\n",
      "   1.23563256e-05  9.04390852e-04  2.14442340e-02  3.66256547e-03\n",
      "   9.33018428e-03 -6.23555048e-03]\n",
      " [ 7.34094276e-03  3.11075698e-03 -1.08572416e-02 -3.23211717e-03\n",
      "  -2.45305209e-03 -6.78095623e-03 -1.62242403e-02  6.78224741e-03\n",
      "  -3.34365709e-02 -1.72028579e-02]\n",
      " [-3.98634401e-03 -7.01048747e-03  1.87973305e-02  6.02887197e-03\n",
      "  -1.34269718e-02  4.88027470e-03  3.42745145e-03  7.13388220e-03\n",
      "   8.18410079e-03 -3.75652456e-03]\n",
      " [-1.32488980e-03  1.08410166e-03  7.71924093e-03 -1.66637698e-03\n",
      "   1.52839131e-05 -1.89113376e-03 -5.73343858e-03  1.17282997e-02\n",
      "  -5.20503048e-03  1.64785603e-02]\n",
      " [-1.07606747e-02  2.36853769e-03  1.02947414e-02 -7.58397064e-03\n",
      "  -1.05961204e-02 -4.23126002e-03 -1.33761409e-03  4.60475343e-03\n",
      "   1.36121748e-02  2.45102084e-03]\n",
      " [ 4.48224852e-03 -1.10728559e-02 -7.23173152e-03 -5.28684926e-03\n",
      "   3.50466355e-02  4.02651789e-03  2.56129629e-03 -1.24360642e-02\n",
      "  -6.85153782e-03  5.41062747e-03]\n",
      " [ 1.41582449e-02  4.41539018e-03  4.81576650e-03  1.94595753e-03\n",
      "   1.83820686e-02  1.08278368e-02  1.33132359e-02  2.49986559e-03\n",
      "   1.60481326e-02 -8.13185346e-03]\n",
      " [-9.37984243e-03 -1.27582821e-02  2.52848364e-03 -1.56392413e-03\n",
      "   4.42164416e-03 -1.39306910e-02 -2.70991739e-03 -1.76612178e-02\n",
      "  -1.17632307e-02 -6.91456871e-03]\n",
      " [ 9.32179548e-03 -1.04408908e-02  1.68832362e-03  2.30697628e-03\n",
      "  -1.53740076e-02 -6.63760843e-03 -1.31427445e-02 -1.70205947e-02\n",
      "  -9.16174348e-03  4.53837895e-03]\n",
      " [ 2.10789311e-02 -2.53134086e-03  1.81259322e-03 -5.08458998e-03\n",
      "  -8.94349082e-03  1.28026284e-03 -1.17566729e-03  9.50657187e-03\n",
      "  -1.18250663e-03 -2.51400854e-03]\n",
      " [-7.95375645e-03 -7.59405933e-03  5.06758232e-03 -8.91579142e-03\n",
      "   6.82166481e-03 -1.77004231e-02 -4.87532833e-03  9.77493019e-03\n",
      "  -4.39621726e-03  1.50754818e-02]\n",
      " [-1.47022672e-02 -2.92034488e-02  1.48518259e-02  1.00264996e-02\n",
      "  -1.77116150e-02 -9.37882881e-03  7.37670845e-03 -1.26235317e-02\n",
      "  -1.97887129e-02 -9.55712277e-03]\n",
      " [ 1.23522608e-02 -7.07948368e-03 -5.91138186e-03 -1.05640379e-02\n",
      "  -1.65984627e-02 -4.38237894e-03 -4.90504469e-03 -2.58434080e-03\n",
      "   7.67052817e-03  1.58666614e-02]\n",
      " [-6.18235706e-03 -2.12941075e-02 -3.76539071e-03 -4.21638173e-04\n",
      "   5.69256607e-03  6.26698503e-03  2.80956735e-03  2.64650070e-03\n",
      "   2.71252514e-03 -6.07763465e-03]\n",
      " [-1.14167094e-03 -1.36295835e-03  2.29309012e-03 -5.41337500e-03\n",
      "   3.84802701e-03 -4.82643779e-03 -3.04131772e-02  3.27250388e-03\n",
      "   4.70832668e-03 -1.03058152e-02]\n",
      " [ 4.44174044e-03 -9.46958865e-03 -6.12910074e-04  6.69216474e-03\n",
      "   2.18698045e-03 -1.69247361e-02 -2.31571004e-02 -8.89247852e-03\n",
      "   8.01773600e-04  4.37033421e-03]\n",
      " [ 6.82460211e-03 -1.19089106e-03  9.95471925e-03 -4.75953930e-03\n",
      "  -7.15480461e-03  1.52588519e-02  9.70209452e-03 -6.72392175e-03\n",
      "  -4.47446515e-03  2.43100249e-02]\n",
      " [ 9.19360956e-05  5.12138045e-03  1.22096126e-02  1.63411880e-03\n",
      "   2.14689351e-03 -1.25761370e-03  4.68510379e-03  8.01568967e-03\n",
      "   6.70186735e-03  2.41944023e-02]\n",
      " [ 4.29351417e-03  4.05305891e-03 -1.76652278e-02 -2.85855988e-02\n",
      "   1.45330064e-03  1.49327225e-02 -9.15281879e-03 -1.42659799e-02\n",
      "   1.00503176e-03  8.80289110e-03]\n",
      " [ 8.52936380e-03  6.00593163e-03  2.15589663e-03  2.15317172e-03\n",
      "  -1.54836071e-02 -7.31956849e-03 -5.34013247e-03 -2.25964493e-03\n",
      "  -3.53522131e-03  1.07527320e-02]\n",
      " [ 1.43745683e-03  8.78504133e-03 -8.67249017e-03 -1.15923385e-02\n",
      "   5.93719040e-03  1.76657485e-02  5.47793367e-03  9.57007204e-03\n",
      "  -3.67519539e-03 -8.55849061e-03]\n",
      " [ 4.52653725e-03 -6.13783753e-03  1.46865807e-02  5.83059343e-03\n",
      "  -7.65973231e-03  1.60418045e-02  1.58287821e-02 -1.04711721e-02\n",
      "  -6.95764739e-03 -7.22751133e-03]\n",
      " [ 7.34719016e-03 -1.33273673e-02 -6.82253164e-03 -4.32533988e-03\n",
      "   9.59964980e-03 -7.79631587e-03 -5.36800498e-03 -1.26780922e-02\n",
      "   1.38904763e-02 -2.64485383e-03]\n",
      " [-7.45554010e-03 -8.49919983e-03  2.33036841e-02 -5.11506082e-03\n",
      "  -6.88076932e-03 -8.78268988e-03  8.44334091e-03  8.64466917e-03\n",
      "   1.16698368e-02  3.32141958e-03]\n",
      " [ 8.61415447e-03  1.89032364e-02 -2.16875061e-03 -2.65401834e-03\n",
      "   2.53061794e-03  1.88374272e-02  3.07528606e-03  4.18238693e-03\n",
      "  -3.10656532e-03 -2.46030672e-03]\n",
      " [ 1.95086159e-02  1.79823605e-02 -2.73247017e-03  6.03647075e-05\n",
      "   3.92683797e-03  4.60085524e-03  9.82011370e-03  1.18036258e-02\n",
      "  -3.00511825e-03  4.34945688e-03]\n",
      " [-6.07749614e-03 -6.87185901e-03 -2.10609424e-03 -8.39359827e-03\n",
      "  -2.09100804e-03 -1.14444621e-03  3.82333752e-03  1.25992921e-02\n",
      "   1.45220520e-02  3.01565683e-03]\n",
      " [ 6.94442805e-03 -1.80882647e-02  1.99459927e-04 -2.70756385e-03\n",
      "  -7.29103558e-03 -6.95258406e-03  1.35610017e-02  8.40348020e-03\n",
      "  -1.84983555e-02  4.20624130e-03]\n",
      " [ 8.95938131e-03  3.61095261e-03 -3.10448271e-03 -1.91243676e-02\n",
      "  -2.75486697e-03 -1.28123978e-02  1.10375278e-02 -7.42852357e-03\n",
      "  -2.58594010e-02  3.02481088e-04]\n",
      " [ 9.03542198e-03  6.43914757e-03  3.13331140e-03 -2.21812149e-02\n",
      "  -1.88867426e-02 -1.00181597e-02 -5.51449347e-03 -9.46180227e-03\n",
      "   6.11299842e-03  7.93632577e-03]\n",
      " [ 9.54548820e-03  2.35891569e-02  9.39088672e-03 -7.79540954e-03\n",
      "  -1.33631726e-02  8.10433004e-03 -8.39826894e-03 -7.88851978e-03\n",
      "   5.14962145e-03 -6.21595300e-03]\n",
      " [ 5.77288278e-03 -6.65205363e-03  1.99951912e-03  8.18124069e-03\n",
      "  -1.42093636e-02  9.80459842e-03  1.00490877e-02 -3.66213674e-03\n",
      "   1.22754448e-02 -1.64929942e-02]\n",
      " [-1.36501875e-02 -8.48404499e-03 -3.76184544e-03  7.67689011e-03\n",
      "   7.24942245e-03  3.64057899e-03 -1.06220564e-02 -5.46694999e-03\n",
      "  -1.04919794e-02  6.90708974e-03]\n",
      " [ 1.82436344e-02 -1.02023562e-02 -1.18641355e-03  1.06013970e-02\n",
      "  -8.94841513e-03  1.75289291e-02  4.99564517e-03  2.24043364e-03\n",
      "   8.84151869e-03 -7.61790424e-03]\n",
      " [ 1.44562704e-03  9.80047145e-03  8.45811112e-03 -2.73904037e-03\n",
      "   2.28649835e-03  1.79138274e-03 -1.55028065e-02 -5.05563105e-04\n",
      "   6.04922718e-04  1.71134621e-03]\n",
      " [-1.04591148e-02 -6.97602036e-03  8.70590388e-03  1.54329006e-03\n",
      "  -1.38663438e-02 -1.67687964e-02  1.59582640e-02  1.27035369e-04\n",
      "  -1.70995331e-02  8.41808449e-03]\n",
      " [ 2.07046076e-03  4.23969858e-04 -2.96305369e-03 -1.12517630e-02\n",
      "   5.80879709e-03  1.52100390e-03 -2.08030546e-02 -1.19802313e-02\n",
      "  -9.07158548e-03 -1.50718897e-02]\n",
      " [-1.84379835e-02 -1.94719749e-02 -1.18637742e-02  4.90295889e-03\n",
      "  -1.06255208e-02 -6.64761142e-03  2.75130551e-02 -1.44571849e-02\n",
      "   7.49295096e-03 -3.43469206e-03]\n",
      " [-1.28817897e-02  1.42853311e-03 -8.05819330e-04 -7.53548367e-03\n",
      "   2.41547414e-03  8.03669342e-03 -5.89781466e-03  6.87903691e-03\n",
      "   2.67421972e-03  2.83895684e-02]\n",
      " [-8.14622988e-03  1.52069305e-02  4.44152061e-03 -5.75606635e-03\n",
      "  -1.31674976e-02  1.06754979e-02  7.66930940e-03 -4.12494426e-03\n",
      "   8.74378672e-03  3.22030361e-03]\n",
      " [-9.67039527e-03  5.71036224e-03 -1.29291820e-02 -6.56730863e-03\n",
      "   6.74175236e-03  9.34607421e-03 -1.52857071e-02  1.51317066e-02\n",
      "  -4.87805512e-03 -2.46153477e-02]\n",
      " [ 2.42924560e-04 -4.60206299e-03  8.63965694e-03 -4.53903651e-03\n",
      "   5.40114377e-03  1.10949660e-02  1.45564365e-02 -1.26735420e-02\n",
      "  -2.78313857e-03 -1.24964687e-02]\n",
      " [-4.62237424e-03  1.31744636e-02  1.25364419e-02  1.99693379e-03\n",
      "   4.68222368e-03  1.39771461e-02 -1.25272618e-02  2.65302184e-03\n",
      "   1.90379891e-02  1.26764946e-02]\n",
      " [-2.41249979e-03  1.36070155e-02  2.73345318e-03  9.12718684e-03\n",
      "   2.67598996e-02 -4.32050673e-03 -2.83177203e-03 -1.22384568e-03\n",
      "  -1.20214996e-03 -5.47192307e-03]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 2\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    print(network.params['W1'])\n",
    "    print(network.params['W2'])\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_train[batch_mask]\n\u001b[0;32m     22\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m t_train[batch_mask]\n\u001b[1;32m---> 24\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     27\u001b[0m     network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grad[key]\n",
      "File \u001b[1;32mc:\\ML\\AI_Study\\DeepLearningFromScratch1\\two_layer_net.py:49\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     46\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     48\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 49\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     51\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\ML\\AI_Study\\DeepLearningFromScratch1\\common\\gradient.py:46\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n",
      "File \u001b[1;32mc:\\ML\\AI_Study\\DeepLearningFromScratch1\\two_layer_net.py:46\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 46\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     49\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\ML\\AI_Study\\DeepLearningFromScratch1\\two_layer_net.py:32\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 32\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "File \u001b[1;32mc:\\ML\\AI_Study\\DeepLearningFromScratch1\\two_layer_net.py:23\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m W1, W2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 23\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[0;32m     24\u001b[0m z1 \u001b[38;5;241m=\u001b[39m sigmoid(a1)\n\u001b[0;32m     25\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, W2) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "import matplotlib.pylab as plt\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "iters_num = 10\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "plt.plot(np.arange(1, len(train_acc_list)+1), train_acc_list)\n",
    "plt.title(\"train accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(1, len(test_acc_list)+1), test_acc_list)\n",
    "plt.title(\"test accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16764857612672377949\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
